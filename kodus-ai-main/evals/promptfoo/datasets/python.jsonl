{"inputs" : {"inputs": {"filePath": "src/queue/async_queue.py", "language": "Python", "fileContent": "import asyncio\nfrom typing import TypeVar, Generic, Callable, Awaitable\nfrom dataclasses import dataclass\nfrom datetime import datetime\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nT = TypeVar(\"T\")\n\n\n@dataclass\nclass QueueItem(Generic[T]):\n    id: str\n    data: T\n    priority: int\n    created_at: datetime\n    attempts: int = 0\n\n\nclass AsyncQueue(Generic[T]):\n    def __init__(self, max_size: int = 1000):\n        self._queue: asyncio.Queue[QueueItem[T]] = asyncio.Queue(maxsize=max_size)\n        self._processing = False\n        self._handlers: list[Callable[[T], Awaitable[None]]] = []\n\n    def add_handler(self, handler: Callable[[T], Awaitable[None]]):\n        self._handlers.append(handler)\n\n    async def enqueue(self, item_id: str, data: T, priority: int = 0) -> bool:\n        try:\n            item = QueueItem(\n                id=item_id,\n                data=data,\n                priority=priority,\n                created_at=datetime.now()\n            )\n            await self._queue.put(item)\n            logger.debug(f\"Enqueued item {item_id}\")\n            return True\n        except asyncio.QueueFull:\n            logger.warning(f\"Queue full, dropping item {item_id}\")\n            return False\n\n    async def process_one(self) -> bool:\n        try:\n            item = await asyncio.wait_for(\n                self._queue.get(),\n                timeout=1.0\n            )\n\n            for handler in self._handlers:\n                handler(item.data)\n\n            self._queue.task_done()\n            return True\n\n        except asyncio.TimeoutError:\n            return False\n        except Exception as e:\n            logger.error(f\"Error processing item: {e}\")\n            return False\n\n    async def start_processing(self):\n        self._processing = True\n        while self._processing:\n            await self.process_one()\n\n    def stop_processing(self):\n        self._processing = False\n\n    def get_size(self) -> int:\n        return self._queue.qsize()\n\n\nclass BatchProcessor:\n    def __init__(self, batch_size: int = 10):\n        self.batch_size = batch_size\n        self._items: list = []\n\n    async def add_item(self, item):\n        self._items.append(item)\n        if len(self._items) >= self.batch_size:\n            await self.flush()\n\n    async def flush(self):\n        if not self._items:\n            return\n\n        batch = self._items\n        self._items = []\n\n        tasks = []\n        for item in batch:\n            task = asyncio.create_task(self._process_item(item))\n            tasks.append(task)\n\n        await asyncio.gather(tasks)\n\n    async def _process_item(self, item):\n        await asyncio.sleep(0.01)\n        logger.info(f\"Processed item: {item}\")\n\n\nasync def run_with_timeout(coro, timeout_seconds: float):\n    try:\n        return await asyncio.wait_for(coro, timeout=timeout_seconds)\n    except asyncio.TimeoutError:\n        logger.warning(\"Operation timed out\")\n        return None\n", "pullRequest": {"id": 9999999999, "url": "https://api.github.com/repos/acme/app/pulls/1", "base": {"ref": "main", "repo": {"fullName": "acme/app"}}, "head": {"ref": "feature-branch", "repo": {"fullName": "acme/app"}}}, "groupingMode": "full", "reviewOptions": {"security": true, "code_style": false, "kody_rules": true, "refactoring": true, "error_handling": true, "maintainability": true, "potential_issues": true, "documentation_and_comments": false}, "limitationType": "pr", "patchWithLinesStr": "## file: 'src/queue/async_queue.py'\n\n@@ -0,0 +1,111 @@\n__new hunk__\n1 +import asyncio\n2 +from typing import TypeVar, Generic, Callable, Awaitable\n3 +from dataclasses import dataclass\n4 +from datetime import datetime\n5 +import logging\n6 +\n7 +logger = logging.getLogger(__name__)\n8 +\n9 +T = TypeVar(\"T\")\n10 +\n11 +\n12 +@dataclass\n13 +class QueueItem(Generic[T]):\n14 +    id: str\n15 +    data: T\n16 +    priority: int\n17 +    created_at: datetime\n18 +    attempts: int = 0\n19 +\n20 +\n21 +class AsyncQueue(Generic[T]):\n22 +    def __init__(self, max_size: int = 1000):\n23 +        self._queue: asyncio.Queue[QueueItem[T]] = asyncio.Queue(maxsize=max_size)\n24 +        self._processing = False\n25 +        self._handlers: list[Callable[[T], Awaitable[None]]] = []\n26 +\n27 +    def add_handler(self, handler: Callable[[T], Awaitable[None]]):\n28 +        self._handlers.append(handler)\n29 +\n30 +    async def enqueue(self, item_id: str, data: T, priority: int = 0) -> bool:\n31 +        try:\n32 +            item = QueueItem(\n33 +                id=item_id,\n34 +                data=data,\n35 +                priority=priority,\n36 +                created_at=datetime.now()\n37 +            )\n38 +            await self._queue.put(item)\n39 +            logger.debug(f\"Enqueued item {item_id}\")\n40 +            return True\n41 +        except asyncio.QueueFull:\n42 +            logger.warning(f\"Queue full, dropping item {item_id}\")\n43 +            return False\n44 +\n45 +    async def process_one(self) -> bool:\n46 +        try:\n47 +            item = await asyncio.wait_for(\n48 +                self._queue.get(),\n49 +                timeout=1.0\n50 +            )\n51 +\n52 +            for handler in self._handlers:\n53 +                handler(item.data)\n54 +\n55 +            self._queue.task_done()\n56 +            return True\n57 +\n58 +        except asyncio.TimeoutError:\n59 +            return False\n60 +        except Exception as e:\n61 +            logger.error(f\"Error processing item: {e}\")\n62 +            return False\n63 +\n64 +    async def start_processing(self):\n65 +        self._processing = True\n66 +        while self._processing:\n67 +            await self.process_one()\n68 +\n69 +    def stop_processing(self):\n70 +        self._processing = False\n71 +\n72 +    def get_size(self) -> int:\n73 +        return self._queue.qsize()\n74 +\n75 +\n76 +class BatchProcessor:\n77 +    def __init__(self, batch_size: int = 10):\n78 +        self.batch_size = batch_size\n79 +        self._items: list = []\n80 +\n81 +    async def add_item(self, item):\n82 +        self._items.append(item)\n83 +        if len(self._items) >= self.batch_size:\n84 +            await self.flush()\n85 +\n86 +    async def flush(self):\n87 +        if not self._items:\n88 +            return\n89 +\n90 +        batch = self._items\n91 +        self._items = []\n92 +\n93 +        tasks = []\n94 +        for item in batch:\n95 +            task = asyncio.create_task(self._process_item(item))\n96 +            tasks.append(task)\n97 +\n98 +        await asyncio.gather(tasks)\n99 +\n100 +    async def _process_item(self, item):\n101 +        await asyncio.sleep(0.01)\n102 +        logger.info(f\"Processed item: {item}\")\n103 +\n104 +\n105 +async def run_with_timeout(coro, timeout_seconds: float):\n106 +    try:\n107 +        return await asyncio.wait_for(coro, timeout=timeout_seconds)\n108 +    except asyncio.TimeoutError:\n109 +        logger.warning(\"Operation timed out\")\n110 +        return None\n111 +\n__old hunk__\n", "severityLevelFilter": "critical", "languageResultPrompt": "en-US", "maxSuggestionsParams": 5, "organizationAndTeamData": {"teamId": "00000000-0000-0000-0000-000000000000", "organizationId": "00000000-0000-0000-0000-000000000001"}}}, "outputs" : {"reference_outputs": {"overallSummary": "Found 2 bug(s) in src/queue/async_queue.py", "codeSuggestions": [{"label": "bug", "language": "Python", "existingCode": "for handler in self._handlers:\n                handler(item.data)", "improvedCode": "for handler in self._handlers:\n                await handler(item.data)", "relevantFile": "src/queue/async_queue.py", "relevantLinesEnd": 51, "suggestionContent": "In `process_one`, the handler is called without `await`: `handler(item.data)`. Since handlers are typed as `Callable[[T], Awaitable[None]]`, they return coroutines that must be awaited. Without `await`, the coroutine is created but never executed, and Python will emit a 'coroutine was never awaited' warning.", "oneSentenceSummary": "Missing await on async handler - coroutine created but never executed", "relevantLinesStart": 50}, {"label": "bug", "language": "Python", "existingCode": "await asyncio.gather(tasks)", "improvedCode": "await asyncio.gather(*tasks)", "relevantFile": "src/queue/async_queue.py", "relevantLinesEnd": 90, "suggestionContent": "In `BatchProcessor.flush`, `asyncio.gather(tasks)` is called but `tasks` is a list, and gather expects individual awaitables as positional arguments. This should be `asyncio.gather(*tasks)` with unpacking. As written, it awaits a single-element tuple containing the list, which doesn't properly parallelize.", "oneSentenceSummary": "Missing * unpacking in asyncio.gather - tasks not parallelized correctly", "relevantLinesStart": 90}]}}, "metadata" : {}}
{"inputs" : {"inputs": {"filePath": "src/tools/git_helper.py", "language": "Python", "fileContent": "import subprocess\nimport os\nfrom pathlib import Path\nfrom dataclasses import dataclass\nfrom typing import Optional\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass GitResult:\n    success: bool\n    output: str\n    error: str | None = None\n\n\nclass GitHelper:\n    def __init__(self, repo_path: str):\n        self.repo_path = Path(repo_path)\n        if not (self.repo_path / \".git\").exists():\n            raise ValueError(f\"Not a git repository: {repo_path}\")\n\n    def run_command(self, command: str) -> GitResult:\n        try:\n            result = subprocess.run(\n                command,\n                shell=True,\n                cwd=self.repo_path,\n                capture_output=True,\n                text=True\n            )\n            return GitResult(\n                success=result.returncode == 0,\n                output=result.stdout.strip(),\n                error=result.stderr.strip() if result.stderr else None\n            )\n        except Exception as e:\n            return GitResult(success=False, output=\"\", error=str(e))\n\n    def get_current_branch(self) -> str:\n        result = self.run_command(\"git branch --show-current\")\n        return result.output if result.success else \"unknown\"\n\n    def checkout(self, branch: str) -> GitResult:\n        return self.run_command(f\"git checkout {branch}\")\n\n    def create_branch(self, branch_name: str) -> GitResult:\n        return self.run_command(f\"git checkout -b {branch_name}\")\n\n    def commit(self, message: str) -> GitResult:\n        return self.run_command(f'git commit -m \"{message}\"')\n\n    def get_log(self, count: int = 10, author: str = None) -> GitResult:\n        cmd = f\"git log -n {count}\"\n        if author:\n            cmd += f' --author=\"{author}\"'\n        return self.run_command(cmd)\n\n    def diff(self, file_path: str = None) -> GitResult:\n        cmd = \"git diff\"\n        if file_path:\n            cmd += f\" -- {file_path}\"\n        return self.run_command(cmd)\n\n    def add_files(self, *files: str) -> GitResult:\n        files_str = \" \".join(files)\n        return self.run_command(f\"git add {files_str}\")\n\n    def get_status(self) -> GitResult:\n        return self.run_command(\"git status --porcelain\")\n\n\ndef clone_repository(url: str, destination: str) -> GitResult:\n    try:\n        result = subprocess.run(\n            f\"git clone {url} {destination}\",\n            shell=True,\n            capture_output=True,\n            text=True\n        )\n        return GitResult(\n            success=result.returncode == 0,\n            output=result.stdout.strip(),\n            error=result.stderr.strip() if result.stderr else None\n        )\n    except Exception as e:\n        return GitResult(success=False, output=\"\", error=str(e))\n", "pullRequest": {"id": 9999999999, "url": "https://api.github.com/repos/acme/app/pulls/1", "base": {"ref": "main", "repo": {"fullName": "acme/app"}}, "head": {"ref": "feature-branch", "repo": {"fullName": "acme/app"}}}, "groupingMode": "full", "reviewOptions": {"security": true, "code_style": false, "kody_rules": true, "refactoring": true, "error_handling": true, "maintainability": true, "potential_issues": true, "documentation_and_comments": false}, "limitationType": "pr", "patchWithLinesStr": "## file: 'src/tools/git_helper.py'\n\n@@ -0,0 +1,89 @@\n__new hunk__\n1 +import subprocess\n2 +import os\n3 +from pathlib import Path\n4 +from dataclasses import dataclass\n5 +from typing import Optional\n6 +import logging\n7 +\n8 +logger = logging.getLogger(__name__)\n9 +\n10 +\n11 +@dataclass\n12 +class GitResult:\n13 +    success: bool\n14 +    output: str\n15 +    error: str | None = None\n16 +\n17 +\n18 +class GitHelper:\n19 +    def __init__(self, repo_path: str):\n20 +        self.repo_path = Path(repo_path)\n21 +        if not (self.repo_path / \".git\").exists():\n22 +            raise ValueError(f\"Not a git repository: {repo_path}\")\n23 +\n24 +    def run_command(self, command: str) -> GitResult:\n25 +        try:\n26 +            result = subprocess.run(\n27 +                command,\n28 +                shell=True,\n29 +                cwd=self.repo_path,\n30 +                capture_output=True,\n31 +                text=True\n32 +            )\n33 +            return GitResult(\n34 +                success=result.returncode == 0,\n35 +                output=result.stdout.strip(),\n36 +                error=result.stderr.strip() if result.stderr else None\n37 +            )\n38 +        except Exception as e:\n39 +            return GitResult(success=False, output=\"\", error=str(e))\n40 +\n41 +    def get_current_branch(self) -> str:\n42 +        result = self.run_command(\"git branch --show-current\")\n43 +        return result.output if result.success else \"unknown\"\n44 +\n45 +    def checkout(self, branch: str) -> GitResult:\n46 +        return self.run_command(f\"git checkout {branch}\")\n47 +\n48 +    def create_branch(self, branch_name: str) -> GitResult:\n49 +        return self.run_command(f\"git checkout -b {branch_name}\")\n50 +\n51 +    def commit(self, message: str) -> GitResult:\n52 +        return self.run_command(f'git commit -m \"{message}\"')\n53 +\n54 +    def get_log(self, count: int = 10, author: str = None) -> GitResult:\n55 +        cmd = f\"git log -n {count}\"\n56 +        if author:\n57 +            cmd += f' --author=\"{author}\"'\n58 +        return self.run_command(cmd)\n59 +\n60 +    def diff(self, file_path: str = None) -> GitResult:\n61 +        cmd = \"git diff\"\n62 +        if file_path:\n63 +            cmd += f\" -- {file_path}\"\n64 +        return self.run_command(cmd)\n65 +\n66 +    def add_files(self, *files: str) -> GitResult:\n67 +        files_str = \" \".join(files)\n68 +        return self.run_command(f\"git add {files_str}\")\n69 +\n70 +    def get_status(self) -> GitResult:\n71 +        return self.run_command(\"git status --porcelain\")\n72 +\n73 +\n74 +def clone_repository(url: str, destination: str) -> GitResult:\n75 +    try:\n76 +        result = subprocess.run(\n77 +            f\"git clone {url} {destination}\",\n78 +            shell=True,\n79 +            capture_output=True,\n80 +            text=True\n81 +        )\n82 +        return GitResult(\n83 +            success=result.returncode == 0,\n84 +            output=result.stdout.strip(),\n85 +            error=result.stderr.strip() if result.stderr else None\n86 +        )\n87 +    except Exception as e:\n88 +        return GitResult(success=False, output=\"\", error=str(e))\n89 +\n__old hunk__\n", "severityLevelFilter": "critical", "languageResultPrompt": "en-US", "maxSuggestionsParams": 5, "organizationAndTeamData": {"teamId": "00000000-0000-0000-0000-000000000000", "organizationId": "00000000-0000-0000-0000-000000000001"}}}, "outputs" : {"reference_outputs": {"overallSummary": "Found 2 bug(s) in src/tools/git_helper.py", "codeSuggestions": [{"label": "bug", "language": "Python", "existingCode": "def checkout(self, branch: str) -> GitResult:\n        return self.run_command(f\"git checkout {branch}\")", "improvedCode": "def checkout(self, branch: str) -> GitResult:\n        return self.run_command_safe([\"git\", \"checkout\", branch])\n\n    def run_command_safe(self, args: list[str]) -> GitResult:\n        result = subprocess.run(args, cwd=self.repo_path, capture_output=True, text=True)\n        return GitResult(success=result.returncode == 0, output=result.stdout.strip())", "relevantFile": "src/tools/git_helper.py", "relevantLinesEnd": 46, "suggestionContent": "Multiple methods pass user input directly to shell commands via `shell=True`. For example, `checkout` passes `branch` directly to the command. An attacker can inject commands like `main; rm -rf /` as the branch name. Use `subprocess.run` with a list of arguments instead of shell=True.", "oneSentenceSummary": "Command injection via shell=True - user input in branch/file names executes arbitrary commands", "relevantLinesStart": 45}, {"label": "bug", "language": "Python", "existingCode": "def commit(self, message: str) -> GitResult:\n        return self.run_command(f'git commit -m \"{message}\"')", "improvedCode": "def commit(self, message: str) -> GitResult:\n        result = subprocess.run(\n            [\"git\", \"commit\", \"-m\", message],\n            cwd=self.repo_path,\n            capture_output=True,\n            text=True\n        )\n        return GitResult(success=result.returncode == 0, output=result.stdout.strip())", "relevantFile": "src/tools/git_helper.py", "relevantLinesEnd": 52, "suggestionContent": "The `commit` method is especially dangerous as it passes the message in quotes, but those quotes can be escaped. Input like `test\"; rm -rf / #` breaks out of the quotes and executes arbitrary commands.", "oneSentenceSummary": "Command injection in commit message - quotes can be escaped to run arbitrary commands", "relevantLinesStart": 51}]}}, "metadata" : {}}
{"inputs" : {"inputs": {"filePath": "src/storage/file_storage.py", "language": "Python", "fileContent": "import os\nimport shutil\nfrom pathlib import Path\nfrom typing import BinaryIO\nfrom datetime import datetime\nimport hashlib\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass FileStorage:\n    def __init__(self, base_path: str):\n        self.base_path = Path(base_path)\n        self.base_path.mkdir(parents=True, exist_ok=True)\n\n    def save_file(self, filename: str, content: bytes) -> str:\n        filepath = self.base_path / filename\n        filepath.parent.mkdir(parents=True, exist_ok=True)\n\n        with open(filepath, \"wb\") as f:\n            f.write(content)\n\n        logger.info(f\"Saved file: {filepath}\")\n        return str(filepath)\n\n    def read_file(self, filename: str) -> bytes:\n        filepath = self.base_path / filename\n\n        if not filepath.exists():\n            raise FileNotFoundError(f\"File not found: {filename}\")\n\n        with open(filepath, \"rb\") as f:\n            return f.read()\n\n    def delete_file(self, filename: str) -> bool:\n        filepath = self.base_path / filename\n\n        if filepath.exists():\n            filepath.unlink()\n            logger.info(f\"Deleted file: {filepath}\")\n            return True\n        return False\n\n    def list_files(self, directory: str = \"\") -> list[str]:\n        search_path = self.base_path / directory\n\n        if not search_path.exists():\n            return []\n\n        return [\n            str(p.relative_to(self.base_path))\n            for p in search_path.rglob(\"*\")\n            if p.is_file()\n        ]\n\n    def get_file_info(self, filename: str) -> dict:\n        filepath = self.base_path / filename\n\n        if not filepath.exists():\n            raise FileNotFoundError(f\"File not found: {filename}\")\n\n        stat = filepath.stat()\n        return {\n            \"name\": filepath.name,\n            \"size\": stat.st_size,\n            \"created\": datetime.fromtimestamp(stat.st_ctime).isoformat(),\n            \"modified\": datetime.fromtimestamp(stat.st_mtime).isoformat(),\n        }\n\n    def copy_file(self, source: str, destination: str) -> str:\n        src_path = self.base_path / source\n        dst_path = self.base_path / destination\n\n        if not src_path.exists():\n            raise FileNotFoundError(f\"Source file not found: {source}\")\n\n        dst_path.parent.mkdir(parents=True, exist_ok=True)\n        shutil.copy2(src_path, dst_path)\n\n        return str(dst_path)\n\n    def move_file(self, source: str, destination: str) -> str:\n        src_path = self.base_path / source\n        dst_path = self.base_path / destination\n\n        if not src_path.exists():\n            raise FileNotFoundError(f\"Source file not found: {source}\")\n\n        dst_path.parent.mkdir(parents=True, exist_ok=True)\n        shutil.move(str(src_path), str(dst_path))\n\n        return str(dst_path)\n\n    def get_file_hash(self, filename: str, algorithm: str = \"sha256\") -> str:\n        filepath = self.base_path / filename\n\n        if not filepath.exists():\n            raise FileNotFoundError(f\"File not found: {filename}\")\n\n        hasher = hashlib.new(algorithm)\n        with open(filepath, \"rb\") as f:\n            for chunk in iter(lambda: f.read(8192), b\"\"):\n                hasher.update(chunk)\n\n        return hasher.hexdigest()\n\n\ndef sanitize_filename(filename: str) -> str:\n    return filename.replace(\"..\", \"\").replace(\"/\", \"_\").replace(\"\\\\\", \"_\")\n", "pullRequest": {"id": 9999999999, "url": "https://api.github.com/repos/acme/app/pulls/1", "base": {"ref": "main", "repo": {"fullName": "acme/app"}}, "head": {"ref": "feature-branch", "repo": {"fullName": "acme/app"}}}, "groupingMode": "full", "reviewOptions": {"security": true, "code_style": false, "kody_rules": true, "refactoring": true, "error_handling": true, "maintainability": true, "potential_issues": true, "documentation_and_comments": false}, "limitationType": "pr", "patchWithLinesStr": "## file: 'src/storage/file_storage.py'\n\n@@ -0,0 +1,111 @@\n__new hunk__\n1 +import os\n2 +import shutil\n3 +from pathlib import Path\n4 +from typing import BinaryIO\n5 +from datetime import datetime\n6 +import hashlib\n7 +import logging\n8 +\n9 +logger = logging.getLogger(__name__)\n10 +\n11 +\n12 +class FileStorage:\n13 +    def __init__(self, base_path: str):\n14 +        self.base_path = Path(base_path)\n15 +        self.base_path.mkdir(parents=True, exist_ok=True)\n16 +\n17 +    def save_file(self, filename: str, content: bytes) -> str:\n18 +        filepath = self.base_path / filename\n19 +        filepath.parent.mkdir(parents=True, exist_ok=True)\n20 +\n21 +        with open(filepath, \"wb\") as f:\n22 +            f.write(content)\n23 +\n24 +        logger.info(f\"Saved file: {filepath}\")\n25 +        return str(filepath)\n26 +\n27 +    def read_file(self, filename: str) -> bytes:\n28 +        filepath = self.base_path / filename\n29 +\n30 +        if not filepath.exists():\n31 +            raise FileNotFoundError(f\"File not found: {filename}\")\n32 +\n33 +        with open(filepath, \"rb\") as f:\n34 +            return f.read()\n35 +\n36 +    def delete_file(self, filename: str) -> bool:\n37 +        filepath = self.base_path / filename\n38 +\n39 +        if filepath.exists():\n40 +            filepath.unlink()\n41 +            logger.info(f\"Deleted file: {filepath}\")\n42 +            return True\n43 +        return False\n44 +\n45 +    def list_files(self, directory: str = \"\") -> list[str]:\n46 +        search_path = self.base_path / directory\n47 +\n48 +        if not search_path.exists():\n49 +            return []\n50 +\n51 +        return [\n52 +            str(p.relative_to(self.base_path))\n53 +            for p in search_path.rglob(\"*\")\n54 +            if p.is_file()\n55 +        ]\n56 +\n57 +    def get_file_info(self, filename: str) -> dict:\n58 +        filepath = self.base_path / filename\n59 +\n60 +        if not filepath.exists():\n61 +            raise FileNotFoundError(f\"File not found: {filename}\")\n62 +\n63 +        stat = filepath.stat()\n64 +        return {\n65 +            \"name\": filepath.name,\n66 +            \"size\": stat.st_size,\n67 +            \"created\": datetime.fromtimestamp(stat.st_ctime).isoformat(),\n68 +            \"modified\": datetime.fromtimestamp(stat.st_mtime).isoformat(),\n69 +        }\n70 +\n71 +    def copy_file(self, source: str, destination: str) -> str:\n72 +        src_path = self.base_path / source\n73 +        dst_path = self.base_path / destination\n74 +\n75 +        if not src_path.exists():\n76 +            raise FileNotFoundError(f\"Source file not found: {source}\")\n77 +\n78 +        dst_path.parent.mkdir(parents=True, exist_ok=True)\n79 +        shutil.copy2(src_path, dst_path)\n80 +\n81 +        return str(dst_path)\n82 +\n83 +    def move_file(self, source: str, destination: str) -> str:\n84 +        src_path = self.base_path / source\n85 +        dst_path = self.base_path / destination\n86 +\n87 +        if not src_path.exists():\n88 +            raise FileNotFoundError(f\"Source file not found: {source}\")\n89 +\n90 +        dst_path.parent.mkdir(parents=True, exist_ok=True)\n91 +        shutil.move(str(src_path), str(dst_path))\n92 +\n93 +        return str(dst_path)\n94 +\n95 +    def get_file_hash(self, filename: str, algorithm: str = \"sha256\") -> str:\n96 +        filepath = self.base_path / filename\n97 +\n98 +        if not filepath.exists():\n99 +            raise FileNotFoundError(f\"File not found: {filename}\")\n100 +\n101 +        hasher = hashlib.new(algorithm)\n102 +        with open(filepath, \"rb\") as f:\n103 +            for chunk in iter(lambda: f.read(8192), b\"\"):\n104 +                hasher.update(chunk)\n105 +\n106 +        return hasher.hexdigest()\n107 +\n108 +\n109 +def sanitize_filename(filename: str) -> str:\n110 +    return filename.replace(\"..\", \"\").replace(\"/\", \"_\").replace(\"\\\\\", \"_\")\n111 +\n__old hunk__\n", "severityLevelFilter": "critical", "languageResultPrompt": "en-US", "maxSuggestionsParams": 5, "organizationAndTeamData": {"teamId": "00000000-0000-0000-0000-000000000000", "organizationId": "00000000-0000-0000-0000-000000000001"}}}, "outputs" : {"reference_outputs": {"overallSummary": "Found 2 bug(s) in src/storage/file_storage.py", "codeSuggestions": [{"label": "bug", "language": "Python", "existingCode": "def read_file(self, filename: str) -> bytes:\n        filepath = self.base_path / filename\n\n        if not filepath.exists():\n            raise FileNotFoundError(f\"File not found: {filename}\")", "improvedCode": "def read_file(self, filename: str) -> bytes:\n        filepath = (self.base_path / filename).resolve()\n\n        if not filepath.is_relative_to(self.base_path.resolve()):\n            raise ValueError(\"Invalid file path\")\n\n        if not filepath.exists():\n            raise FileNotFoundError(f\"File not found: {filename}\")", "relevantFile": "src/storage/file_storage.py", "relevantLinesEnd": 30, "suggestionContent": "All file operations (`save_file`, `read_file`, `delete_file`, etc.) are vulnerable to path traversal attacks. An attacker can pass `../../../etc/passwd` as filename to access files outside the base directory. Using `Path(base_path) / filename` doesn't prevent `..` sequences. Need to resolve the path and verify it's still under base_path.", "oneSentenceSummary": "Path traversal vulnerability allows reading files outside storage directory", "relevantLinesStart": 26}, {"label": "bug", "language": "Python", "existingCode": "def sanitize_filename(filename: str) -> str:\n    return filename.replace(\"..\", \"\").replace(\"/\", \"_\").replace(\"\\\\\\\\\", \"_\")", "improvedCode": "def sanitize_filename(filename: str) -> str:\n    # Remove any path components, keep only the final filename\n    from pathlib import PurePath\n    return PurePath(filename).name", "relevantFile": "src/storage/file_storage.py", "relevantLinesEnd": 104, "suggestionContent": "The `sanitize_filename` function is insufficient - it removes `..` but an attacker can use `....//` which becomes `../` after replacement. Also, it doesn't handle URL-encoded sequences like `%2e%2e%2f`. Path resolution and containment check is the proper solution.", "oneSentenceSummary": "Sanitize function can be bypassed with ....// or URL-encoded sequences", "relevantLinesStart": 103}]}}, "metadata" : {}}
{"inputs" : {"inputs": {"filePath": "src/services/notification_service.py", "language": "Python", "fileContent": "from dataclasses import dataclass\nfrom datetime import datetime\nfrom typing import Optional\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass Notification:\n    id: str\n    user_id: str\n    message: str\n    created_at: datetime\n    read: bool = False\n    metadata: dict = None\n\n\nclass NotificationService:\n    def __init__(self, max_notifications: int = 100):\n        self.max_notifications = max_notifications\n        self._notifications: dict[str, list[Notification]] = {}\n\n    def create_notification(\n        self,\n        user_id: str,\n        message: str,\n        metadata: dict = {}\n    ) -> Notification:\n        notification = Notification(\n            id=f\"notif_{datetime.now().timestamp()}\",\n            user_id=user_id,\n            message=message,\n            created_at=datetime.now(),\n            metadata=metadata\n        )\n\n        if user_id not in self._notifications:\n            self._notifications[user_id] = []\n\n        user_notifications = self._notifications[user_id]\n\n        if len(user_notifications) >= self.max_notifications:\n            user_notifications.pop(0)\n\n        user_notifications.append(notification)\n        logger.info(f\"Created notification for user {user_id}\")\n\n        return notification\n\n    def get_notifications(self, user_id: str) -> list[Notification]:\n        return self._notifications.get(user_id, [])\n\n    def mark_as_read(self, user_id: str, notification_id: str) -> bool:\n        notifications = self._notifications.get(user_id, [])\n        for notif in notifications:\n            if notif.id == notification_id:\n                notif.read = True\n                return True\n        return False\n\n    def get_unread_count(self, user_id: str) -> int:\n        notifications = self._notifications.get(user_id, [])\n        return sum(1 for n in notifications if not n.read)\n\n    def clear_notifications(self, user_id: str) -> int:\n        if user_id in self._notifications:\n            count = len(self._notifications[user_id])\n            self._notifications[user_id] = []\n            return count\n        return 0\n\n\ndef create_bulk_notifications(\n    service: NotificationService,\n    user_ids: list[str],\n    message: str,\n    tags: list = []\n) -> list[Notification]:\n    notifications = []\n    for user_id in user_ids:\n        notif = service.create_notification(\n            user_id=user_id,\n            message=message,\n            metadata={\"tags\": tags}\n        )\n        notifications.append(notif)\n    return notifications\n", "pullRequest": {"id": 9999999999, "url": "https://api.github.com/repos/acme/app/pulls/1", "base": {"ref": "main", "repo": {"fullName": "acme/app"}}, "head": {"ref": "feature-branch", "repo": {"fullName": "acme/app"}}}, "groupingMode": "full", "reviewOptions": {"security": true, "code_style": false, "kody_rules": true, "refactoring": true, "error_handling": true, "maintainability": true, "potential_issues": true, "documentation_and_comments": false}, "limitationType": "pr", "patchWithLinesStr": "## file: 'src/services/notification_service.py'\n\n@@ -0,0 +1,89 @@\n__new hunk__\n1 +from dataclasses import dataclass\n2 +from datetime import datetime\n3 +from typing import Optional\n4 +import logging\n5 +\n6 +logger = logging.getLogger(__name__)\n7 +\n8 +\n9 +@dataclass\n10 +class Notification:\n11 +    id: str\n12 +    user_id: str\n13 +    message: str\n14 +    created_at: datetime\n15 +    read: bool = False\n16 +    metadata: dict = None\n17 +\n18 +\n19 +class NotificationService:\n20 +    def __init__(self, max_notifications: int = 100):\n21 +        self.max_notifications = max_notifications\n22 +        self._notifications: dict[str, list[Notification]] = {}\n23 +\n24 +    def create_notification(\n25 +        self,\n26 +        user_id: str,\n27 +        message: str,\n28 +        metadata: dict = {}\n29 +    ) -> Notification:\n30 +        notification = Notification(\n31 +            id=f\"notif_{datetime.now().timestamp()}\",\n32 +            user_id=user_id,\n33 +            message=message,\n34 +            created_at=datetime.now(),\n35 +            metadata=metadata\n36 +        )\n37 +\n38 +        if user_id not in self._notifications:\n39 +            self._notifications[user_id] = []\n40 +\n41 +        user_notifications = self._notifications[user_id]\n42 +\n43 +        if len(user_notifications) >= self.max_notifications:\n44 +            user_notifications.pop(0)\n45 +\n46 +        user_notifications.append(notification)\n47 +        logger.info(f\"Created notification for user {user_id}\")\n48 +\n49 +        return notification\n50 +\n51 +    def get_notifications(self, user_id: str) -> list[Notification]:\n52 +        return self._notifications.get(user_id, [])\n53 +\n54 +    def mark_as_read(self, user_id: str, notification_id: str) -> bool:\n55 +        notifications = self._notifications.get(user_id, [])\n56 +        for notif in notifications:\n57 +            if notif.id == notification_id:\n58 +                notif.read = True\n59 +                return True\n60 +        return False\n61 +\n62 +    def get_unread_count(self, user_id: str) -> int:\n63 +        notifications = self._notifications.get(user_id, [])\n64 +        return sum(1 for n in notifications if not n.read)\n65 +\n66 +    def clear_notifications(self, user_id: str) -> int:\n67 +        if user_id in self._notifications:\n68 +            count = len(self._notifications[user_id])\n69 +            self._notifications[user_id] = []\n70 +            return count\n71 +        return 0\n72 +\n73 +\n74 +def create_bulk_notifications(\n75 +    service: NotificationService,\n76 +    user_ids: list[str],\n77 +    message: str,\n78 +    tags: list = []\n79 +) -> list[Notification]:\n80 +    notifications = []\n81 +    for user_id in user_ids:\n82 +        notif = service.create_notification(\n83 +            user_id=user_id,\n84 +            message=message,\n85 +            metadata={\"tags\": tags}\n86 +        )\n87 +        notifications.append(notif)\n88 +    return notifications\n89 +\n__old hunk__\n", "severityLevelFilter": "critical", "languageResultPrompt": "en-US", "maxSuggestionsParams": 5, "organizationAndTeamData": {"teamId": "00000000-0000-0000-0000-000000000000", "organizationId": "00000000-0000-0000-0000-000000000001"}}}, "outputs" : {"reference_outputs": {"overallSummary": "Found 2 bug(s) in src/services/notification_service.py", "codeSuggestions": [{"label": "bug", "language": "Python", "existingCode": "def create_notification(\n        self,\n        user_id: str,\n        message: str,\n        metadata: dict = {}\n    ) -> Notification:", "improvedCode": "def create_notification(\n        self,\n        user_id: str,\n        message: str,\n        metadata: dict | None = None\n    ) -> Notification:\n        if metadata is None:\n            metadata = {}", "relevantFile": "src/services/notification_service.py", "relevantLinesEnd": 31, "suggestionContent": "The `create_notification` method uses a mutable default argument `metadata: dict = {}`. In Python, default arguments are evaluated once when the function is defined, not each time it's called. All calls that don't provide metadata will share the same dict object, causing unexpected data sharing between notifications.", "oneSentenceSummary": "Mutable default argument causes shared state between function calls", "relevantLinesStart": 26}, {"label": "bug", "language": "Python", "existingCode": "def create_bulk_notifications(\n    service: NotificationService,\n    user_ids: list[str],\n    message: str,\n    tags: list = []\n) -> list[Notification]:", "improvedCode": "def create_bulk_notifications(\n    service: NotificationService,\n    user_ids: list[str],\n    message: str,\n    tags: list | None = None\n) -> list[Notification]:\n    if tags is None:\n        tags = []", "relevantFile": "src/services/notification_service.py", "relevantLinesEnd": 84, "suggestionContent": "The `create_bulk_notifications` function has the same mutable default argument bug with `tags: list = []`. Additionally, since the same `tags` list is passed to all notifications' metadata, modifying tags in one notification will affect all others.", "oneSentenceSummary": "Mutable default argument for tags list shared across all bulk notifications", "relevantLinesStart": 79}]}}, "metadata" : {}}
{"inputs" : {"inputs": {"filePath": "src/data/data_processor.py", "language": "Python", "fileContent": "from typing import Iterator, Iterable, TypeVar, Callable\nfrom dataclasses import dataclass\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nT = TypeVar(\"T\")\nR = TypeVar(\"R\")\n\n\n@dataclass\nclass ProcessingResult:\n    total: int\n    processed: int\n    failed: int\n    results: list\n\n\ndef process_items(items: Iterator[T], processor: Callable[[T], R]) -> ProcessingResult:\n    total = sum(1 for _ in items)\n\n    processed = 0\n    failed = 0\n    results = []\n\n    for item in items:\n        try:\n            result = processor(item)\n            results.append(result)\n            processed += 1\n        except Exception as e:\n            logger.error(f\"Failed to process item: {e}\")\n            failed += 1\n\n    return ProcessingResult(\n        total=total,\n        processed=processed,\n        failed=failed,\n        results=results\n    )\n\n\ndef filter_and_transform(\n    items: Iterator[T],\n    filter_fn: Callable[[T], bool],\n    transform_fn: Callable[[T], R]\n) -> list[R]:\n    filtered = (item for item in items if filter_fn(item))\n\n    if not any(filtered):\n        logger.info(\"No items passed filter\")\n        return []\n\n    return [transform_fn(item) for item in filtered]\n\n\ndef batch_process(items: Iterator[T], batch_size: int) -> Iterator[list[T]]:\n    batch = []\n    for item in items:\n        batch.append(item)\n        if len(batch) >= batch_size:\n            yield batch\n            batch = []\n    if batch:\n        yield batch\n\n\ndef get_statistics(numbers: Iterator[float]) -> dict:\n    numbers_list = list(numbers)\n\n    if not numbers_list:\n        return {\"count\": 0, \"sum\": 0, \"avg\": 0, \"min\": 0, \"max\": 0}\n\n    return {\n        \"count\": len(numbers_list),\n        \"sum\": sum(numbers_list),\n        \"avg\": sum(numbers_list) / len(numbers_list),\n        \"min\": min(numbers_list),\n        \"max\": max(numbers_list)\n    }\n\n\nclass DataPipeline:\n    def __init__(self):\n        self.steps: list[Callable] = []\n\n    def add_step(self, step: Callable):\n        self.steps.append(step)\n        return self\n\n    def execute(self, data: Iterable[T]) -> list:\n        current = iter(data)\n        for step in self.steps:\n            current = step(current)\n        return list(current)\n\n\ndef validate_all(items: Iterator[T], validator: Callable[[T], bool]) -> tuple[bool, list[T]]:\n    invalid_items = [item for item in items if not validator(item)]\n\n    all_valid = len(invalid_items) == 0\n\n    valid_items = [item for item in items if validator(item)]\n\n    return all_valid, valid_items\n", "pullRequest": {"id": 9999999999, "url": "https://api.github.com/repos/acme/app/pulls/1", "base": {"ref": "main", "repo": {"fullName": "acme/app"}}, "head": {"ref": "feature-branch", "repo": {"fullName": "acme/app"}}}, "groupingMode": "full", "reviewOptions": {"security": true, "code_style": false, "kody_rules": true, "refactoring": true, "error_handling": true, "maintainability": true, "potential_issues": true, "documentation_and_comments": false}, "limitationType": "pr", "patchWithLinesStr": "## file: 'src/data/data_processor.py'\n\n@@ -0,0 +1,106 @@\n__new hunk__\n1 +from typing import Iterator, Iterable, TypeVar, Callable\n2 +from dataclasses import dataclass\n3 +import logging\n4 +\n5 +logger = logging.getLogger(__name__)\n6 +\n7 +T = TypeVar(\"T\")\n8 +R = TypeVar(\"R\")\n9 +\n10 +\n11 +@dataclass\n12 +class ProcessingResult:\n13 +    total: int\n14 +    processed: int\n15 +    failed: int\n16 +    results: list\n17 +\n18 +\n19 +def process_items(items: Iterator[T], processor: Callable[[T], R]) -> ProcessingResult:\n20 +    total = sum(1 for _ in items)\n21 +\n22 +    processed = 0\n23 +    failed = 0\n24 +    results = []\n25 +\n26 +    for item in items:\n27 +        try:\n28 +            result = processor(item)\n29 +            results.append(result)\n30 +            processed += 1\n31 +        except Exception as e:\n32 +            logger.error(f\"Failed to process item: {e}\")\n33 +            failed += 1\n34 +\n35 +    return ProcessingResult(\n36 +        total=total,\n37 +        processed=processed,\n38 +        failed=failed,\n39 +        results=results\n40 +    )\n41 +\n42 +\n43 +def filter_and_transform(\n44 +    items: Iterator[T],\n45 +    filter_fn: Callable[[T], bool],\n46 +    transform_fn: Callable[[T], R]\n47 +) -> list[R]:\n48 +    filtered = (item for item in items if filter_fn(item))\n49 +\n50 +    if not any(filtered):\n51 +        logger.info(\"No items passed filter\")\n52 +        return []\n53 +\n54 +    return [transform_fn(item) for item in filtered]\n55 +\n56 +\n57 +def batch_process(items: Iterator[T], batch_size: int) -> Iterator[list[T]]:\n58 +    batch = []\n59 +    for item in items:\n60 +        batch.append(item)\n61 +        if len(batch) >= batch_size:\n62 +            yield batch\n63 +            batch = []\n64 +    if batch:\n65 +        yield batch\n66 +\n67 +\n68 +def get_statistics(numbers: Iterator[float]) -> dict:\n69 +    numbers_list = list(numbers)\n70 +\n71 +    if not numbers_list:\n72 +        return {\"count\": 0, \"sum\": 0, \"avg\": 0, \"min\": 0, \"max\": 0}\n73 +\n74 +    return {\n75 +        \"count\": len(numbers_list),\n76 +        \"sum\": sum(numbers_list),\n77 +        \"avg\": sum(numbers_list) / len(numbers_list),\n78 +        \"min\": min(numbers_list),\n79 +        \"max\": max(numbers_list)\n80 +    }\n81 +\n82 +\n83 +class DataPipeline:\n84 +    def __init__(self):\n85 +        self.steps: list[Callable] = []\n86 +\n87 +    def add_step(self, step: Callable):\n88 +        self.steps.append(step)\n89 +        return self\n90 +\n91 +    def execute(self, data: Iterable[T]) -> list:\n92 +        current = iter(data)\n93 +        for step in self.steps:\n94 +            current = step(current)\n95 +        return list(current)\n96 +\n97 +\n98 +def validate_all(items: Iterator[T], validator: Callable[[T], bool]) -> tuple[bool, list[T]]:\n99 +    invalid_items = [item for item in items if not validator(item)]\n100 +\n101 +    all_valid = len(invalid_items) == 0\n102 +\n103 +    valid_items = [item for item in items if validator(item)]\n104 +\n105 +    return all_valid, valid_items\n106 +\n__old hunk__\n", "severityLevelFilter": "critical", "languageResultPrompt": "en-US", "maxSuggestionsParams": 5, "organizationAndTeamData": {"teamId": "00000000-0000-0000-0000-000000000000", "organizationId": "00000000-0000-0000-0000-000000000001"}}}, "outputs" : {"reference_outputs": {"overallSummary": "Found 3 bug(s) in src/data/data_processor.py", "codeSuggestions": [{"label": "bug", "language": "Python", "existingCode": "def process_items(items: Iterator[T], processor: Callable[[T], R]) -> ProcessingResult:\n    total = sum(1 for _ in items)\n\n    processed = 0\n    failed = 0\n    results = []\n\n    for item in items:", "improvedCode": "def process_items(items: Iterator[T], processor: Callable[[T], R]) -> ProcessingResult:\n    items_list = list(items)\n    total = len(items_list)\n\n    processed = 0\n    failed = 0\n    results = []\n\n    for item in items_list:", "relevantFile": "src/data/data_processor.py", "relevantLinesEnd": 26, "suggestionContent": "In `process_items`, the iterator `items` is consumed by `sum(1 for _ in items)` to count total items. When the for loop tries to iterate over `items` again, it's already exhausted and yields nothing. The function will always report 0 processed items. Convert to list first or use itertools.tee.", "oneSentenceSummary": "Iterator exhausted by sum() - second iteration processes zero items", "relevantLinesStart": 19}, {"label": "bug", "language": "Python", "existingCode": "filtered = (item for item in items if filter_fn(item))\n\n    if not any(filtered):\n        logger.info(\"No items passed filter\")\n        return []\n\n    return [transform_fn(item) for item in filtered]", "improvedCode": "filtered = [item for item in items if filter_fn(item)]\n\n    if not filtered:\n        logger.info(\"No items passed filter\")\n        return []\n\n    return [transform_fn(item) for item in filtered]", "relevantFile": "src/data/data_processor.py", "relevantLinesEnd": 50, "suggestionContent": "In `filter_and_transform`, the generator `filtered` is consumed by `any(filtered)` to check if items exist. Then the list comprehension tries to iterate over the same exhausted generator, producing an empty list even when items passed the filter.", "oneSentenceSummary": "Generator exhausted by any() check - returns empty list for valid data", "relevantLinesStart": 44}, {"label": "bug", "language": "Python", "existingCode": "invalid_items = [item for item in items if not validator(item)]\n\n    all_valid = len(invalid_items) == 0\n\n    valid_items = [item for item in items if validator(item)]", "improvedCode": "items_list = list(items)\n    invalid_items = [item for item in items_list if not validator(item)]\n\n    all_valid = len(invalid_items) == 0\n\n    valid_items = [item for item in items_list if validator(item)]", "relevantFile": "src/data/data_processor.py", "relevantLinesEnd": 98, "suggestionContent": "In `validate_all`, the iterator `items` is consumed by the first list comprehension to find invalid items. The second comprehension iterates over the same exhausted iterator, always producing an empty `valid_items` list.", "oneSentenceSummary": "Iterator consumed twice - valid_items always empty after invalid_items check", "relevantLinesStart": 94}]}}, "metadata" : {}}
{"inputs" : {"inputs": {"filePath": "src/cache/serialization_cache.py", "language": "Python", "fileContent": "import pickle\nimport json\nimport zlib\nfrom typing import Any, TypeVar\nfrom pathlib import Path\nfrom datetime import datetime, timedelta\nimport hashlib\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nT = TypeVar(\"T\")\n\n\nclass SerializationCache:\n    def __init__(self, cache_dir: str = \"./cache\"):\n        self.cache_dir = Path(cache_dir)\n        self.cache_dir.mkdir(parents=True, exist_ok=True)\n\n    def _get_cache_path(self, key: str) -> Path:\n        key_hash = hashlib.sha256(key.encode()).hexdigest()[:16]\n        return self.cache_dir / f\"{key_hash}.cache\"\n\n    def set(self, key: str, value: Any, compress: bool = False) -> None:\n        cache_path = self._get_cache_path(key)\n\n        data = pickle.dumps(value)\n\n        if compress:\n            data = zlib.compress(data)\n\n        cache_path.write_bytes(data)\n        logger.debug(f\"Cached value for key: {key}\")\n\n    def get(self, key: str, decompress: bool = False) -> Any | None:\n        cache_path = self._get_cache_path(key)\n\n        if not cache_path.exists():\n            return None\n\n        data = cache_path.read_bytes()\n\n        if decompress:\n            data = zlib.decompress(data)\n\n        return pickle.loads(data)\n\n    def delete(self, key: str) -> bool:\n        cache_path = self._get_cache_path(key)\n        if cache_path.exists():\n            cache_path.unlink()\n            return True\n        return False\n\n    def clear(self) -> int:\n        count = 0\n        for cache_file in self.cache_dir.glob(\"*.cache\"):\n            cache_file.unlink()\n            count += 1\n        return count\n\n\nclass RemoteCache:\n    def __init__(self, local_cache: SerializationCache):\n        self.local_cache = local_cache\n        self._remote_data: dict[str, bytes] = {}\n\n    def fetch_and_cache(self, key: str, remote_data: bytes) -> Any:\n        value = pickle.loads(remote_data)\n        self.local_cache.set(key, value)\n        return value\n\n    def export_to_remote(self, key: str) -> bytes:\n        value = self.local_cache.get(key)\n        if value is None:\n            raise KeyError(f\"Key not found: {key}\")\n        return pickle.dumps(value)\n\n    def import_from_bytes(self, data: bytes) -> Any:\n        return pickle.loads(data)\n\n\nclass CacheSerializer:\n    @staticmethod\n    def serialize(value: Any, format: str = \"pickle\") -> bytes:\n        if format == \"pickle\":\n            return pickle.dumps(value)\n        elif format == \"json\":\n            return json.dumps(value).encode()\n        else:\n            raise ValueError(f\"Unsupported format: {format}\")\n\n    @staticmethod\n    def deserialize(data: bytes, format: str = \"pickle\") -> Any:\n        if format == \"pickle\":\n            return pickle.loads(data)\n        elif format == \"json\":\n            return json.loads(data.decode())\n        else:\n            raise ValueError(f\"Unsupported format: {format}\")\n", "pullRequest": {"id": 9999999999, "url": "https://api.github.com/repos/acme/app/pulls/1", "base": {"ref": "main", "repo": {"fullName": "acme/app"}}, "head": {"ref": "feature-branch", "repo": {"fullName": "acme/app"}}}, "groupingMode": "full", "reviewOptions": {"security": true, "code_style": false, "kody_rules": true, "refactoring": true, "error_handling": true, "maintainability": true, "potential_issues": true, "documentation_and_comments": false}, "limitationType": "pr", "patchWithLinesStr": "## file: 'src/cache/serialization_cache.py'\n\n@@ -0,0 +1,101 @@\n__new hunk__\n1 +import pickle\n2 +import json\n3 +import zlib\n4 +from typing import Any, TypeVar\n5 +from pathlib import Path\n6 +from datetime import datetime, timedelta\n7 +import hashlib\n8 +import logging\n9 +\n10 +logger = logging.getLogger(__name__)\n11 +\n12 +T = TypeVar(\"T\")\n13 +\n14 +\n15 +class SerializationCache:\n16 +    def __init__(self, cache_dir: str = \"./cache\"):\n17 +        self.cache_dir = Path(cache_dir)\n18 +        self.cache_dir.mkdir(parents=True, exist_ok=True)\n19 +\n20 +    def _get_cache_path(self, key: str) -> Path:\n21 +        key_hash = hashlib.sha256(key.encode()).hexdigest()[:16]\n22 +        return self.cache_dir / f\"{key_hash}.cache\"\n23 +\n24 +    def set(self, key: str, value: Any, compress: bool = False) -> None:\n25 +        cache_path = self._get_cache_path(key)\n26 +\n27 +        data = pickle.dumps(value)\n28 +\n29 +        if compress:\n30 +            data = zlib.compress(data)\n31 +\n32 +        cache_path.write_bytes(data)\n33 +        logger.debug(f\"Cached value for key: {key}\")\n34 +\n35 +    def get(self, key: str, decompress: bool = False) -> Any | None:\n36 +        cache_path = self._get_cache_path(key)\n37 +\n38 +        if not cache_path.exists():\n39 +            return None\n40 +\n41 +        data = cache_path.read_bytes()\n42 +\n43 +        if decompress:\n44 +            data = zlib.decompress(data)\n45 +\n46 +        return pickle.loads(data)\n47 +\n48 +    def delete(self, key: str) -> bool:\n49 +        cache_path = self._get_cache_path(key)\n50 +        if cache_path.exists():\n51 +            cache_path.unlink()\n52 +            return True\n53 +        return False\n54 +\n55 +    def clear(self) -> int:\n56 +        count = 0\n57 +        for cache_file in self.cache_dir.glob(\"*.cache\"):\n58 +            cache_file.unlink()\n59 +            count += 1\n60 +        return count\n61 +\n62 +\n63 +class RemoteCache:\n64 +    def __init__(self, local_cache: SerializationCache):\n65 +        self.local_cache = local_cache\n66 +        self._remote_data: dict[str, bytes] = {}\n67 +\n68 +    def fetch_and_cache(self, key: str, remote_data: bytes) -> Any:\n69 +        value = pickle.loads(remote_data)\n70 +        self.local_cache.set(key, value)\n71 +        return value\n72 +\n73 +    def export_to_remote(self, key: str) -> bytes:\n74 +        value = self.local_cache.get(key)\n75 +        if value is None:\n76 +            raise KeyError(f\"Key not found: {key}\")\n77 +        return pickle.dumps(value)\n78 +\n79 +    def import_from_bytes(self, data: bytes) -> Any:\n80 +        return pickle.loads(data)\n81 +\n82 +\n83 +class CacheSerializer:\n84 +    @staticmethod\n85 +    def serialize(value: Any, format: str = \"pickle\") -> bytes:\n86 +        if format == \"pickle\":\n87 +            return pickle.dumps(value)\n88 +        elif format == \"json\":\n89 +            return json.dumps(value).encode()\n90 +        else:\n91 +            raise ValueError(f\"Unsupported format: {format}\")\n92 +\n93 +    @staticmethod\n94 +    def deserialize(data: bytes, format: str = \"pickle\") -> Any:\n95 +        if format == \"pickle\":\n96 +            return pickle.loads(data)\n97 +        elif format == \"json\":\n98 +            return json.loads(data.decode())\n99 +        else:\n100 +            raise ValueError(f\"Unsupported format: {format}\")\n101 +\n__old hunk__\n", "severityLevelFilter": "critical", "languageResultPrompt": "en-US", "maxSuggestionsParams": 5, "organizationAndTeamData": {"teamId": "00000000-0000-0000-0000-000000000000", "organizationId": "00000000-0000-0000-0000-000000000001"}}}, "outputs" : {"reference_outputs": {"overallSummary": "Found 1 bug(s) in src/cache/serialization_cache.py", "codeSuggestions": [{"label": "bug", "language": "Python", "existingCode": "def fetch_and_cache(self, key: str, remote_data: bytes) -> Any:\n        value = pickle.loads(remote_data)\n        self.local_cache.set(key, value)\n        return value", "improvedCode": "def fetch_and_cache(self, key: str, remote_data: bytes) -> Any:\n        # Use JSON for untrusted data, or implement a RestrictedUnpickler\n        import json\n        value = json.loads(remote_data.decode())\n        self.local_cache.set(key, value)\n        return value", "relevantFile": "src/cache/serialization_cache.py", "relevantLinesEnd": 68, "suggestionContent": "Multiple methods use `pickle.loads()` to deserialize data, including data from external sources (`fetch_and_cache`, `import_from_bytes`). Pickle can execute arbitrary code during deserialization. An attacker can craft a malicious pickle payload that runs system commands when loaded. Never unpickle data from untrusted sources.", "oneSentenceSummary": "Pickle deserialization of untrusted data allows arbitrary code execution", "relevantLinesStart": 65}]}}, "metadata" : {}}
{"inputs" : {"inputs": {"filePath": "src/database/user_repository.py", "language": "Python", "fileContent": "from dataclasses import dataclass\nfrom datetime import datetime\nfrom typing import Optional\nimport sqlite3\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass User:\n    id: int\n    username: str\n    email: str\n    created_at: datetime\n    is_active: bool = True\n\n\nclass UserRepository:\n    def __init__(self, db_path: str):\n        self.db_path = db_path\n        self._connection: sqlite3.Connection | None = None\n\n    def _get_connection(self) -> sqlite3.Connection:\n        if self._connection is None:\n            self._connection = sqlite3.connect(self.db_path)\n            self._connection.row_factory = sqlite3.Row\n        return self._connection\n\n    def find_by_id(self, user_id: int) -> User | None:\n        conn = self._get_connection()\n        cursor = conn.execute(\n            \"SELECT * FROM users WHERE id = ?\",\n            (user_id,)\n        )\n        row = cursor.fetchone()\n        return self._row_to_user(row) if row else None\n\n    def find_by_username(self, username: str) -> User | None:\n        conn = self._get_connection()\n        query = f\"SELECT * FROM users WHERE username = '{username}'\"\n        cursor = conn.execute(query)\n        row = cursor.fetchone()\n        return self._row_to_user(row) if row else None\n\n    def find_by_email(self, email: str) -> User | None:\n        conn = self._get_connection()\n        cursor = conn.execute(\n            \"SELECT * FROM users WHERE email = ?\",\n            (email,)\n        )\n        row = cursor.fetchone()\n        return self._row_to_user(row) if row else None\n\n    def search_users(self, search_term: str, field: str = \"username\") -> list[User]:\n        conn = self._get_connection()\n        query = f\"SELECT * FROM users WHERE {field} LIKE '%{search_term}%'\"\n        cursor = conn.execute(query)\n        return [self._row_to_user(row) for row in cursor.fetchall()]\n\n    def create_user(self, username: str, email: str) -> User:\n        conn = self._get_connection()\n        cursor = conn.execute(\n            \"INSERT INTO users (username, email, created_at) VALUES (?, ?, ?)\",\n            (username, email, datetime.now().isoformat())\n        )\n        conn.commit()\n        return self.find_by_id(cursor.lastrowid)\n\n    def update_user(self, user_id: int, **fields) -> bool:\n        if not fields:\n            return False\n\n        conn = self._get_connection()\n        set_clause = \", \".join(f\"{k} = ?\" for k in fields.keys())\n        values = list(fields.values()) + [user_id]\n\n        conn.execute(\n            f\"UPDATE users SET {set_clause} WHERE id = ?\",\n            values\n        )\n        conn.commit()\n        return True\n\n    def delete_user(self, user_id: int) -> bool:\n        conn = self._get_connection()\n        cursor = conn.execute(\n            \"DELETE FROM users WHERE id = ?\",\n            (user_id,)\n        )\n        conn.commit()\n        return cursor.rowcount > 0\n\n    def get_users_by_status(self, is_active: bool, order_by: str = \"created_at\") -> list[User]:\n        conn = self._get_connection()\n        query = f\"SELECT * FROM users WHERE is_active = ? ORDER BY {order_by}\"\n        cursor = conn.execute(query, (is_active,))\n        return [self._row_to_user(row) for row in cursor.fetchall()]\n\n    def _row_to_user(self, row: sqlite3.Row) -> User:\n        return User(\n            id=row[\"id\"],\n            username=row[\"username\"],\n            email=row[\"email\"],\n            created_at=datetime.fromisoformat(row[\"created_at\"]),\n            is_active=bool(row[\"is_active\"])\n        )\n\n    def close(self):\n        if self._connection:\n            self._connection.close()\n            self._connection = None\n", "pullRequest": {"id": 9999999999, "url": "https://api.github.com/repos/acme/app/pulls/1", "base": {"ref": "main", "repo": {"fullName": "acme/app"}}, "head": {"ref": "feature-branch", "repo": {"fullName": "acme/app"}}}, "groupingMode": "full", "reviewOptions": {"security": true, "code_style": false, "kody_rules": true, "refactoring": true, "error_handling": true, "maintainability": true, "potential_issues": true, "documentation_and_comments": false}, "limitationType": "pr", "patchWithLinesStr": "## file: 'src/database/user_repository.py'\n\n@@ -0,0 +1,113 @@\n__new hunk__\n1 +from dataclasses import dataclass\n2 +from datetime import datetime\n3 +from typing import Optional\n4 +import sqlite3\n5 +import logging\n6 +\n7 +logger = logging.getLogger(__name__)\n8 +\n9 +\n10 +@dataclass\n11 +class User:\n12 +    id: int\n13 +    username: str\n14 +    email: str\n15 +    created_at: datetime\n16 +    is_active: bool = True\n17 +\n18 +\n19 +class UserRepository:\n20 +    def __init__(self, db_path: str):\n21 +        self.db_path = db_path\n22 +        self._connection: sqlite3.Connection | None = None\n23 +\n24 +    def _get_connection(self) -> sqlite3.Connection:\n25 +        if self._connection is None:\n26 +            self._connection = sqlite3.connect(self.db_path)\n27 +            self._connection.row_factory = sqlite3.Row\n28 +        return self._connection\n29 +\n30 +    def find_by_id(self, user_id: int) -> User | None:\n31 +        conn = self._get_connection()\n32 +        cursor = conn.execute(\n33 +            \"SELECT * FROM users WHERE id = ?\",\n34 +            (user_id,)\n35 +        )\n36 +        row = cursor.fetchone()\n37 +        return self._row_to_user(row) if row else None\n38 +\n39 +    def find_by_username(self, username: str) -> User | None:\n40 +        conn = self._get_connection()\n41 +        query = f\"SELECT * FROM users WHERE username = '{username}'\"\n42 +        cursor = conn.execute(query)\n43 +        row = cursor.fetchone()\n44 +        return self._row_to_user(row) if row else None\n45 +\n46 +    def find_by_email(self, email: str) -> User | None:\n47 +        conn = self._get_connection()\n48 +        cursor = conn.execute(\n49 +            \"SELECT * FROM users WHERE email = ?\",\n50 +            (email,)\n51 +        )\n52 +        row = cursor.fetchone()\n53 +        return self._row_to_user(row) if row else None\n54 +\n55 +    def search_users(self, search_term: str, field: str = \"username\") -> list[User]:\n56 +        conn = self._get_connection()\n57 +        query = f\"SELECT * FROM users WHERE {field} LIKE '%{search_term}%'\"\n58 +        cursor = conn.execute(query)\n59 +        return [self._row_to_user(row) for row in cursor.fetchall()]\n60 +\n61 +    def create_user(self, username: str, email: str) -> User:\n62 +        conn = self._get_connection()\n63 +        cursor = conn.execute(\n64 +            \"INSERT INTO users (username, email, created_at) VALUES (?, ?, ?)\",\n65 +            (username, email, datetime.now().isoformat())\n66 +        )\n67 +        conn.commit()\n68 +        return self.find_by_id(cursor.lastrowid)\n69 +\n70 +    def update_user(self, user_id: int, **fields) -> bool:\n71 +        if not fields:\n72 +            return False\n73 +\n74 +        conn = self._get_connection()\n75 +        set_clause = \", \".join(f\"{k} = ?\" for k in fields.keys())\n76 +        values = list(fields.values()) + [user_id]\n77 +\n78 +        conn.execute(\n79 +            f\"UPDATE users SET {set_clause} WHERE id = ?\",\n80 +            values\n81 +        )\n82 +        conn.commit()\n83 +        return True\n84 +\n85 +    def delete_user(self, user_id: int) -> bool:\n86 +        conn = self._get_connection()\n87 +        cursor = conn.execute(\n88 +            \"DELETE FROM users WHERE id = ?\",\n89 +            (user_id,)\n90 +        )\n91 +        conn.commit()\n92 +        return cursor.rowcount > 0\n93 +\n94 +    def get_users_by_status(self, is_active: bool, order_by: str = \"created_at\") -> list[User]:\n95 +        conn = self._get_connection()\n96 +        query = f\"SELECT * FROM users WHERE is_active = ? ORDER BY {order_by}\"\n97 +        cursor = conn.execute(query, (is_active,))\n98 +        return [self._row_to_user(row) for row in cursor.fetchall()]\n99 +\n100 +    def _row_to_user(self, row: sqlite3.Row) -> User:\n101 +        return User(\n102 +            id=row[\"id\"],\n103 +            username=row[\"username\"],\n104 +            email=row[\"email\"],\n105 +            created_at=datetime.fromisoformat(row[\"created_at\"]),\n106 +            is_active=bool(row[\"is_active\"])\n107 +        )\n108 +\n109 +    def close(self):\n110 +        if self._connection:\n111 +            self._connection.close()\n112 +            self._connection = None\n113 +\n__old hunk__\n", "severityLevelFilter": "critical", "languageResultPrompt": "en-US", "maxSuggestionsParams": 5, "organizationAndTeamData": {"teamId": "00000000-0000-0000-0000-000000000000", "organizationId": "00000000-0000-0000-0000-000000000001"}}}, "outputs" : {"reference_outputs": {"overallSummary": "Found 3 bug(s) in src/database/user_repository.py", "codeSuggestions": [{"label": "bug", "language": "Python", "existingCode": "query = f\"SELECT * FROM users WHERE username = '{username}'\"", "improvedCode": "cursor = conn.execute(\n            \"SELECT * FROM users WHERE username = ?\",\n            (username,)\n        )", "relevantFile": "src/database/user_repository.py", "relevantLinesEnd": 40, "suggestionContent": "The `find_by_username` method uses f-string formatting to build SQL query, making it vulnerable to SQL injection. An attacker can pass `' OR '1'='1` as username to bypass authentication or `'; DROP TABLE users; --` to delete data. Use parameterized queries instead.", "oneSentenceSummary": "SQL injection vulnerability - username directly interpolated into query", "relevantLinesStart": 40}, {"label": "bug", "language": "Python", "existingCode": "query = f\"SELECT * FROM users WHERE {field} LIKE '%{search_term}%'\"", "improvedCode": "allowed_fields = {'username', 'email'}\n        if field not in allowed_fields:\n            raise ValueError(f\"Invalid field: {field}\")\n        query = f\"SELECT * FROM users WHERE {field} LIKE ?\"\n        cursor = conn.execute(query, (f'%{search_term}%',))", "relevantFile": "src/database/user_repository.py", "relevantLinesEnd": 54, "suggestionContent": "The `search_users` method has two SQL injection vulnerabilities: both `field` and `search_term` are interpolated directly into the query. An attacker can inject via the field parameter (e.g., `username; DROP TABLE users; --`) or the search term.", "oneSentenceSummary": "SQL injection via field name and search term parameters", "relevantLinesStart": 54}, {"label": "bug", "language": "Python", "existingCode": "query = f\"SELECT * FROM users WHERE is_active = ? ORDER BY {order_by}\"", "improvedCode": "allowed_order_fields = {'created_at', 'username', 'email', 'id'}\n        if order_by not in allowed_order_fields:\n            raise ValueError(f\"Invalid order field: {order_by}\")\n        query = f\"SELECT * FROM users WHERE is_active = ? ORDER BY {order_by}\"", "relevantFile": "src/database/user_repository.py", "relevantLinesEnd": 88, "suggestionContent": "The `get_users_by_status` method interpolates `order_by` directly into the query without validation. An attacker can inject SQL via the order_by parameter, e.g., `created_at; DROP TABLE users; --`.", "oneSentenceSummary": "SQL injection via order_by parameter not validated", "relevantLinesStart": 88}]}}, "metadata" : {}}
{"inputs" : {"inputs": {"filePath": "src/session/session_manager.py", "language": "Python", "fileContent": "from datetime import datetime, timedelta\nfrom typing import Any\nfrom dataclasses import dataclass, field\nimport secrets\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass Session:\n    id: str\n    user_id: str\n    data: dict[str, Any]\n    created_at: datetime\n    last_accessed: datetime\n    expires_at: datetime\n\n    def is_expired(self) -> bool:\n        return datetime.now() > self.expires_at\n\n    def touch(self):\n        self.last_accessed = datetime.now()\n\n\nclass SessionManager:\n    def __init__(self, session_ttl_minutes: int = 30):\n        self._sessions: dict[str, Session] = {}\n        self._user_sessions: dict[str, set[str]] = {}\n        self._ttl = timedelta(minutes=session_ttl_minutes)\n\n    def create_session(self, user_id: str, data: dict = None) -> Session:\n        session_id = secrets.token_urlsafe(32)\n        now = datetime.now()\n\n        session = Session(\n            id=session_id,\n            user_id=user_id,\n            data=data or {},\n            created_at=now,\n            last_accessed=now,\n            expires_at=now + self._ttl\n        )\n\n        self._sessions[session_id] = session\n\n        if user_id not in self._user_sessions:\n            self._user_sessions[user_id] = set()\n        self._user_sessions[user_id].add(session_id)\n\n        logger.info(f\"Created session {session_id} for user {user_id}\")\n        return session\n\n    def get_session(self, session_id: str) -> Session | None:\n        session = self._sessions.get(session_id)\n        if session and not session.is_expired():\n            session.touch()\n            return session\n        return None\n\n    def delete_session(self, session_id: str) -> bool:\n        session = self._sessions.get(session_id)\n        if session:\n            del self._sessions[session_id]\n            if session.user_id in self._user_sessions:\n                self._user_sessions[session.user_id].discard(session_id)\n            return True\n        return False\n\n    def cleanup_expired_sessions(self) -> int:\n        count = 0\n        for session_id, session in self._sessions.items():\n            if session.is_expired():\n                del self._sessions[session_id]\n                if session.user_id in self._user_sessions:\n                    self._user_sessions[session.user_id].discard(session_id)\n                count += 1\n                logger.info(f\"Cleaned up expired session {session_id}\")\n        return count\n\n    def get_user_sessions(self, user_id: str) -> list[Session]:\n        session_ids = self._user_sessions.get(user_id, set())\n        sessions = []\n        for sid in session_ids:\n            session = self.get_session(sid)\n            if session:\n                sessions.append(session)\n        return sessions\n\n    def invalidate_user_sessions(self, user_id: str) -> int:\n        session_ids = self._user_sessions.get(user_id, set())\n        count = 0\n        for session_id in session_ids:\n            if self.delete_session(session_id):\n                count += 1\n        return count\n\n    def get_active_session_count(self) -> int:\n        return sum(1 for s in self._sessions.values() if not s.is_expired())\n\n    def get_stats(self) -> dict:\n        total = len(self._sessions)\n        active = self.get_active_session_count()\n        return {\n            \"total_sessions\": total,\n            \"active_sessions\": active,\n            \"expired_sessions\": total - active,\n            \"unique_users\": len(self._user_sessions)\n        }\n", "pullRequest": {"id": 9999999999, "url": "https://api.github.com/repos/acme/app/pulls/1", "base": {"ref": "main", "repo": {"fullName": "acme/app"}}, "head": {"ref": "feature-branch", "repo": {"fullName": "acme/app"}}}, "groupingMode": "full", "reviewOptions": {"security": true, "code_style": false, "kody_rules": true, "refactoring": true, "error_handling": true, "maintainability": true, "potential_issues": true, "documentation_and_comments": false}, "limitationType": "pr", "patchWithLinesStr": "## file: 'src/session/session_manager.py'\n\n@@ -0,0 +1,110 @@\n__new hunk__\n1 +from datetime import datetime, timedelta\n2 +from typing import Any\n3 +from dataclasses import dataclass, field\n4 +import secrets\n5 +import logging\n6 +\n7 +logger = logging.getLogger(__name__)\n8 +\n9 +\n10 +@dataclass\n11 +class Session:\n12 +    id: str\n13 +    user_id: str\n14 +    data: dict[str, Any]\n15 +    created_at: datetime\n16 +    last_accessed: datetime\n17 +    expires_at: datetime\n18 +\n19 +    def is_expired(self) -> bool:\n20 +        return datetime.now() > self.expires_at\n21 +\n22 +    def touch(self):\n23 +        self.last_accessed = datetime.now()\n24 +\n25 +\n26 +class SessionManager:\n27 +    def __init__(self, session_ttl_minutes: int = 30):\n28 +        self._sessions: dict[str, Session] = {}\n29 +        self._user_sessions: dict[str, set[str]] = {}\n30 +        self._ttl = timedelta(minutes=session_ttl_minutes)\n31 +\n32 +    def create_session(self, user_id: str, data: dict = None) -> Session:\n33 +        session_id = secrets.token_urlsafe(32)\n34 +        now = datetime.now()\n35 +\n36 +        session = Session(\n37 +            id=session_id,\n38 +            user_id=user_id,\n39 +            data=data or {},\n40 +            created_at=now,\n41 +            last_accessed=now,\n42 +            expires_at=now + self._ttl\n43 +        )\n44 +\n45 +        self._sessions[session_id] = session\n46 +\n47 +        if user_id not in self._user_sessions:\n48 +            self._user_sessions[user_id] = set()\n49 +        self._user_sessions[user_id].add(session_id)\n50 +\n51 +        logger.info(f\"Created session {session_id} for user {user_id}\")\n52 +        return session\n53 +\n54 +    def get_session(self, session_id: str) -> Session | None:\n55 +        session = self._sessions.get(session_id)\n56 +        if session and not session.is_expired():\n57 +            session.touch()\n58 +            return session\n59 +        return None\n60 +\n61 +    def delete_session(self, session_id: str) -> bool:\n62 +        session = self._sessions.get(session_id)\n63 +        if session:\n64 +            del self._sessions[session_id]\n65 +            if session.user_id in self._user_sessions:\n66 +                self._user_sessions[session.user_id].discard(session_id)\n67 +            return True\n68 +        return False\n69 +\n70 +    def cleanup_expired_sessions(self) -> int:\n71 +        count = 0\n72 +        for session_id, session in self._sessions.items():\n73 +            if session.is_expired():\n74 +                del self._sessions[session_id]\n75 +                if session.user_id in self._user_sessions:\n76 +                    self._user_sessions[session.user_id].discard(session_id)\n77 +                count += 1\n78 +                logger.info(f\"Cleaned up expired session {session_id}\")\n79 +        return count\n80 +\n81 +    def get_user_sessions(self, user_id: str) -> list[Session]:\n82 +        session_ids = self._user_sessions.get(user_id, set())\n83 +        sessions = []\n84 +        for sid in session_ids:\n85 +            session = self.get_session(sid)\n86 +            if session:\n87 +                sessions.append(session)\n88 +        return sessions\n89 +\n90 +    def invalidate_user_sessions(self, user_id: str) -> int:\n91 +        session_ids = self._user_sessions.get(user_id, set())\n92 +        count = 0\n93 +        for session_id in session_ids:\n94 +            if self.delete_session(session_id):\n95 +                count += 1\n96 +        return count\n97 +\n98 +    def get_active_session_count(self) -> int:\n99 +        return sum(1 for s in self._sessions.values() if not s.is_expired())\n100 +\n101 +    def get_stats(self) -> dict:\n102 +        total = len(self._sessions)\n103 +        active = self.get_active_session_count()\n104 +        return {\n105 +            \"total_sessions\": total,\n106 +            \"active_sessions\": active,\n107 +            \"expired_sessions\": total - active,\n108 +            \"unique_users\": len(self._user_sessions)\n109 +        }\n110 +\n__old hunk__\n", "severityLevelFilter": "critical", "languageResultPrompt": "en-US", "maxSuggestionsParams": 5, "organizationAndTeamData": {"teamId": "00000000-0000-0000-0000-000000000000", "organizationId": "00000000-0000-0000-0000-000000000001"}}}, "outputs" : {"reference_outputs": {"overallSummary": "Found 2 bug(s) in src/session/session_manager.py", "codeSuggestions": [{"label": "bug", "language": "Python", "existingCode": "def cleanup_expired_sessions(self) -> int:\n        count = 0\n        for session_id, session in self._sessions.items():\n            if session.is_expired():\n                del self._sessions[session_id]", "improvedCode": "def cleanup_expired_sessions(self) -> int:\n        count = 0\n        expired_ids = [\n            sid for sid, session in self._sessions.items()\n            if session.is_expired()\n        ]\n        for session_id in expired_ids:\n            session = self._sessions[session_id]\n            del self._sessions[session_id]", "relevantFile": "src/session/session_manager.py", "relevantLinesEnd": 76, "suggestionContent": "The `cleanup_expired_sessions` method modifies `self._sessions` dict while iterating over it with `.items()`. This raises `RuntimeError: dictionary changed size during iteration` in Python 3. Create a list of keys to delete first, then delete them in a separate loop.", "oneSentenceSummary": "Dictionary modified during iteration causes RuntimeError", "relevantLinesStart": 68}, {"label": "bug", "language": "Python", "existingCode": "def invalidate_user_sessions(self, user_id: str) -> int:\n        session_ids = self._user_sessions.get(user_id, set())\n        count = 0\n        for session_id in session_ids:\n            if self.delete_session(session_id):\n                count += 1\n        return count", "improvedCode": "def invalidate_user_sessions(self, user_id: str) -> int:\n        session_ids = list(self._user_sessions.get(user_id, set()))\n        count = 0\n        for session_id in session_ids:\n            if self.delete_session(session_id):\n                count += 1\n        return count", "relevantFile": "src/session/session_manager.py", "relevantLinesEnd": 92, "suggestionContent": "The `invalidate_user_sessions` method iterates over `session_ids` set while `delete_session` modifies the same set via `self._user_sessions[session.user_id].discard(session_id)`. This causes `RuntimeError: Set changed size during iteration`.", "oneSentenceSummary": "Set modified during iteration via delete_session causes RuntimeError", "relevantLinesStart": 86}]}}, "metadata" : {}}
{"inputs" : {"inputs": {"filePath": "src/workers/task_scheduler.py", "language": "Python", "fileContent": "import asyncio\nfrom datetime import datetime, timedelta\nfrom typing import Callable, Awaitable\nfrom dataclasses import dataclass, field\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass ScheduledTask:\n    id: str\n    name: str\n    callback: Callable[[], Awaitable[None]]\n    interval_seconds: int\n    last_run: datetime | None = None\n    is_running: bool = False\n    run_count: int = 0\n\n\nclass TaskScheduler:\n    def __init__(self):\n        self.tasks: dict[str, ScheduledTask] = {}\n        self._running = False\n        self._task_handles: list[asyncio.Task] = []\n\n    def add_task(\n        self,\n        task_id: str,\n        name: str,\n        callback: Callable[[], Awaitable[None]],\n        interval_seconds: int\n    ) -> ScheduledTask:\n        task = ScheduledTask(\n            id=task_id,\n            name=name,\n            callback=callback,\n            interval_seconds=interval_seconds\n        )\n        self.tasks[task_id] = task\n        logger.info(f\"Added task: {name} (interval: {interval_seconds}s)\")\n        return task\n\n    async def _run_task(self, task: ScheduledTask):\n        while self._running:\n            try:\n                task.is_running = True\n                await task.callback()\n                task.last_run = datetime.now()\n                task.run_count += 1\n            except Exception as e:\n                logger.error(f\"Task {task.name} failed: {e}\")\n            finally:\n                task.is_running = False\n\n            await asyncio.sleep(task.interval_seconds)\n\n    async def start(self):\n        if self._running:\n            return\n\n        self._running = True\n        logger.info(\"Starting task scheduler\")\n\n        for task_id in self.tasks:\n            task = self.tasks[task_id]\n            handle = asyncio.create_task(self._run_task(task))\n            self._task_handles.append(handle)\n\n    async def stop(self):\n        self._running = False\n        for handle in self._task_handles:\n            handle.cancel()\n        self._task_handles.clear()\n        logger.info(\"Task scheduler stopped\")\n\n    def get_task_status(self, task_id: str) -> dict | None:\n        task = self.tasks.get(task_id)\n        if not task:\n            return None\n        return {\n            \"id\": task.id,\n            \"name\": task.name,\n            \"is_running\": task.is_running,\n            \"run_count\": task.run_count,\n            \"last_run\": task.last_run.isoformat() if task.last_run else None\n        }\n\n\ndef create_scheduled_tasks(scheduler: TaskScheduler, task_configs: list[dict]):\n    callbacks = []\n\n    for config in task_configs:\n        async def task_callback():\n            logger.info(f\"Executing task: {config['name']}\")\n            await asyncio.sleep(0.1)\n\n        callbacks.append(task_callback)\n\n        scheduler.add_task(\n            task_id=config[\"id\"],\n            name=config[\"name\"],\n            callback=task_callback,\n            interval_seconds=config[\"interval\"]\n        )\n\n\nasync def run_parallel_tasks(tasks: list[Callable[[], Awaitable[None]]]):\n    handles = []\n    for i, task in enumerate(tasks):\n        handle = asyncio.create_task(task())\n        handles.append(handle)\n\n    await asyncio.gather(*handles)\n", "pullRequest": {"id": 9999999999, "url": "https://api.github.com/repos/acme/app/pulls/1", "base": {"ref": "main", "repo": {"fullName": "acme/app"}}, "head": {"ref": "feature-branch", "repo": {"fullName": "acme/app"}}}, "groupingMode": "full", "reviewOptions": {"security": true, "code_style": false, "kody_rules": true, "refactoring": true, "error_handling": true, "maintainability": true, "potential_issues": true, "documentation_and_comments": false}, "limitationType": "pr", "patchWithLinesStr": "## file: 'src/workers/task_scheduler.py'\n\n@@ -0,0 +1,115 @@\n__new hunk__\n1 +import asyncio\n2 +from datetime import datetime, timedelta\n3 +from typing import Callable, Awaitable\n4 +from dataclasses import dataclass, field\n5 +import logging\n6 +\n7 +logger = logging.getLogger(__name__)\n8 +\n9 +\n10 +@dataclass\n11 +class ScheduledTask:\n12 +    id: str\n13 +    name: str\n14 +    callback: Callable[[], Awaitable[None]]\n15 +    interval_seconds: int\n16 +    last_run: datetime | None = None\n17 +    is_running: bool = False\n18 +    run_count: int = 0\n19 +\n20 +\n21 +class TaskScheduler:\n22 +    def __init__(self):\n23 +        self.tasks: dict[str, ScheduledTask] = {}\n24 +        self._running = False\n25 +        self._task_handles: list[asyncio.Task] = []\n26 +\n27 +    def add_task(\n28 +        self,\n29 +        task_id: str,\n30 +        name: str,\n31 +        callback: Callable[[], Awaitable[None]],\n32 +        interval_seconds: int\n33 +    ) -> ScheduledTask:\n34 +        task = ScheduledTask(\n35 +            id=task_id,\n36 +            name=name,\n37 +            callback=callback,\n38 +            interval_seconds=interval_seconds\n39 +        )\n40 +        self.tasks[task_id] = task\n41 +        logger.info(f\"Added task: {name} (interval: {interval_seconds}s)\")\n42 +        return task\n43 +\n44 +    async def _run_task(self, task: ScheduledTask):\n45 +        while self._running:\n46 +            try:\n47 +                task.is_running = True\n48 +                await task.callback()\n49 +                task.last_run = datetime.now()\n50 +                task.run_count += 1\n51 +            except Exception as e:\n52 +                logger.error(f\"Task {task.name} failed: {e}\")\n53 +            finally:\n54 +                task.is_running = False\n55 +\n56 +            await asyncio.sleep(task.interval_seconds)\n57 +\n58 +    async def start(self):\n59 +        if self._running:\n60 +            return\n61 +\n62 +        self._running = True\n63 +        logger.info(\"Starting task scheduler\")\n64 +\n65 +        for task_id in self.tasks:\n66 +            task = self.tasks[task_id]\n67 +            handle = asyncio.create_task(self._run_task(task))\n68 +            self._task_handles.append(handle)\n69 +\n70 +    async def stop(self):\n71 +        self._running = False\n72 +        for handle in self._task_handles:\n73 +            handle.cancel()\n74 +        self._task_handles.clear()\n75 +        logger.info(\"Task scheduler stopped\")\n76 +\n77 +    def get_task_status(self, task_id: str) -> dict | None:\n78 +        task = self.tasks.get(task_id)\n79 +        if not task:\n80 +            return None\n81 +        return {\n82 +            \"id\": task.id,\n83 +            \"name\": task.name,\n84 +            \"is_running\": task.is_running,\n85 +            \"run_count\": task.run_count,\n86 +            \"last_run\": task.last_run.isoformat() if task.last_run else None\n87 +        }\n88 +\n89 +\n90 +def create_scheduled_tasks(scheduler: TaskScheduler, task_configs: list[dict]):\n91 +    callbacks = []\n92 +\n93 +    for config in task_configs:\n94 +        async def task_callback():\n95 +            logger.info(f\"Executing task: {config['name']}\")\n96 +            await asyncio.sleep(0.1)\n97 +\n98 +        callbacks.append(task_callback)\n99 +\n100 +        scheduler.add_task(\n101 +            task_id=config[\"id\"],\n102 +            name=config[\"name\"],\n103 +            callback=task_callback,\n104 +            interval_seconds=config[\"interval\"]\n105 +        )\n106 +\n107 +\n108 +async def run_parallel_tasks(tasks: list[Callable[[], Awaitable[None]]]):\n109 +    handles = []\n110 +    for i, task in enumerate(tasks):\n111 +        handle = asyncio.create_task(task())\n112 +        handles.append(handle)\n113 +\n114 +    await asyncio.gather(*handles)\n115 +\n__old hunk__\n", "severityLevelFilter": "critical", "languageResultPrompt": "en-US", "maxSuggestionsParams": 5, "organizationAndTeamData": {"teamId": "00000000-0000-0000-0000-000000000000", "organizationId": "00000000-0000-0000-0000-000000000001"}}}, "outputs" : {"reference_outputs": {"overallSummary": "Found 1 bug(s) in src/workers/task_scheduler.py", "codeSuggestions": [{"label": "bug", "language": "Python", "existingCode": "for config in task_configs:\n        async def task_callback():\n            logger.info(f\"Executing task: {config['name']}\")\n            await asyncio.sleep(0.1)", "improvedCode": "for config in task_configs:\n        async def task_callback(cfg=config):  # Capture by default argument\n            logger.info(f\"Executing task: {cfg['name']}\")\n            await asyncio.sleep(0.1)", "relevantFile": "src/workers/task_scheduler.py", "relevantLinesEnd": 90, "suggestionContent": "The `create_scheduled_tasks` function has a late binding closure bug. The `task_callback` function captures the variable `config` by reference, not by value. By the time any callback executes, the loop has finished and `config` refers to the last item in `task_configs`. All tasks will log the same name (the last config's name).", "oneSentenceSummary": "Late binding closure captures loop variable by reference - all callbacks use last value", "relevantLinesStart": 87}]}}, "metadata" : {}}
{"inputs" : {"inputs": {"filePath": "src/cache/memory_cache.py", "language": "Python", "fileContent": "from datetime import datetime, timedelta\nfrom typing import Any, Generic, TypeVar\nfrom dataclasses import dataclass\nimport threading\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nT = TypeVar(\"T\")\n\nCACHE_MISS = object()\n\n\n@dataclass\nclass CacheEntry(Generic[T]):\n    value: T\n    expires_at: datetime | None\n    created_at: datetime\n    access_count: int = 0\n\n\nclass MemoryCache(Generic[T]):\n    def __init__(self, default_ttl_seconds: int = 300):\n        self._store: dict[str, CacheEntry[T]] = {}\n        self._default_ttl = default_ttl_seconds\n        self._lock = threading.RLock()\n        self._stats = {\"hits\": 0, \"misses\": 0}\n\n    def get(self, key: str, default: T = None) -> T | None:\n        with self._lock:\n            entry = self._store.get(key)\n\n            if entry is None:\n                self._stats[\"misses\"] += 1\n                return default\n\n            if entry.expires_at and datetime.now() > entry.expires_at:\n                del self._store[key]\n                self._stats[\"misses\"] += 1\n                return default\n\n            entry.access_count += 1\n            self._stats[\"hits\"] += 1\n            return entry.value\n\n    def set(\n        self,\n        key: str,\n        value: T,\n        ttl_seconds: int | None = None\n    ) -> None:\n        with self._lock:\n            ttl = ttl_seconds if ttl_seconds is not None else self._default_ttl\n            expires_at = datetime.now() + timedelta(seconds=ttl) if ttl else None\n\n            self._store[key] = CacheEntry(\n                value=value,\n                expires_at=expires_at,\n                created_at=datetime.now()\n            )\n\n    def delete(self, key: str) -> bool:\n        with self._lock:\n            if key in self._store:\n                del self._store[key]\n                return True\n            return False\n\n    def clear(self) -> int:\n        with self._lock:\n            count = len(self._store)\n            self._store.clear()\n            return count\n\n    def get_stats(self) -> dict:\n        return self._stats.copy()\n\n\nclass CacheManager:\n    def __init__(self):\n        self._caches: dict[str, MemoryCache] = {}\n\n    def get_cache(self, namespace: str) -> MemoryCache:\n        if namespace not in self._caches:\n            self._caches[namespace] = MemoryCache()\n        return self._caches[namespace]\n\n    def get_or_compute(\n        self,\n        namespace: str,\n        key: str,\n        compute_fn,\n        ttl_seconds: int = 300\n    ) -> Any:\n        cache = self.get_cache(namespace)\n        value = cache.get(key, default=CACHE_MISS)\n\n        if value is CACHE_MISS:\n            value = compute_fn()\n            cache.set(key, value, ttl_seconds)\n\n        return value\n\n    def invalidate_namespace(self, namespace: str) -> int:\n        if namespace in self._caches:\n            return self._caches[namespace].clear()\n        return 0\n\n\ndef cached_result(key: str, result: Any, expected: Any) -> bool:\n    if result is None and expected is None:\n        return True\n    if result is expected:\n        return True\n    return result == expected\n\n\ndef check_cache_value(cache: MemoryCache, key: str, expected_value: int) -> bool:\n    value = cache.get(key)\n    if value is expected_value:\n        return True\n    if value is None:\n        return False\n    return value == expected_value\n", "pullRequest": {"id": 9999999999, "url": "https://api.github.com/repos/acme/app/pulls/1", "base": {"ref": "main", "repo": {"fullName": "acme/app"}}, "head": {"ref": "feature-branch", "repo": {"fullName": "acme/app"}}}, "groupingMode": "full", "reviewOptions": {"security": true, "code_style": false, "kody_rules": true, "refactoring": true, "error_handling": true, "maintainability": true, "potential_issues": true, "documentation_and_comments": false}, "limitationType": "pr", "patchWithLinesStr": "## file: 'src/cache/memory_cache.py'\n\n@@ -0,0 +1,125 @@\n__new hunk__\n1 +from datetime import datetime, timedelta\n2 +from typing import Any, Generic, TypeVar\n3 +from dataclasses import dataclass\n4 +import threading\n5 +import logging\n6 +\n7 +logger = logging.getLogger(__name__)\n8 +\n9 +T = TypeVar(\"T\")\n10 +\n11 +CACHE_MISS = object()\n12 +\n13 +\n14 +@dataclass\n15 +class CacheEntry(Generic[T]):\n16 +    value: T\n17 +    expires_at: datetime | None\n18 +    created_at: datetime\n19 +    access_count: int = 0\n20 +\n21 +\n22 +class MemoryCache(Generic[T]):\n23 +    def __init__(self, default_ttl_seconds: int = 300):\n24 +        self._store: dict[str, CacheEntry[T]] = {}\n25 +        self._default_ttl = default_ttl_seconds\n26 +        self._lock = threading.RLock()\n27 +        self._stats = {\"hits\": 0, \"misses\": 0}\n28 +\n29 +    def get(self, key: str, default: T = None) -> T | None:\n30 +        with self._lock:\n31 +            entry = self._store.get(key)\n32 +\n33 +            if entry is None:\n34 +                self._stats[\"misses\"] += 1\n35 +                return default\n36 +\n37 +            if entry.expires_at and datetime.now() > entry.expires_at:\n38 +                del self._store[key]\n39 +                self._stats[\"misses\"] += 1\n40 +                return default\n41 +\n42 +            entry.access_count += 1\n43 +            self._stats[\"hits\"] += 1\n44 +            return entry.value\n45 +\n46 +    def set(\n47 +        self,\n48 +        key: str,\n49 +        value: T,\n50 +        ttl_seconds: int | None = None\n51 +    ) -> None:\n52 +        with self._lock:\n53 +            ttl = ttl_seconds if ttl_seconds is not None else self._default_ttl\n54 +            expires_at = datetime.now() + timedelta(seconds=ttl) if ttl else None\n55 +\n56 +            self._store[key] = CacheEntry(\n57 +                value=value,\n58 +                expires_at=expires_at,\n59 +                created_at=datetime.now()\n60 +            )\n61 +\n62 +    def delete(self, key: str) -> bool:\n63 +        with self._lock:\n64 +            if key in self._store:\n65 +                del self._store[key]\n66 +                return True\n67 +            return False\n68 +\n69 +    def clear(self) -> int:\n70 +        with self._lock:\n71 +            count = len(self._store)\n72 +            self._store.clear()\n73 +            return count\n74 +\n75 +    def get_stats(self) -> dict:\n76 +        return self._stats.copy()\n77 +\n78 +\n79 +class CacheManager:\n80 +    def __init__(self):\n81 +        self._caches: dict[str, MemoryCache] = {}\n82 +\n83 +    def get_cache(self, namespace: str) -> MemoryCache:\n84 +        if namespace not in self._caches:\n85 +            self._caches[namespace] = MemoryCache()\n86 +        return self._caches[namespace]\n87 +\n88 +    def get_or_compute(\n89 +        self,\n90 +        namespace: str,\n91 +        key: str,\n92 +        compute_fn,\n93 +        ttl_seconds: int = 300\n94 +    ) -> Any:\n95 +        cache = self.get_cache(namespace)\n96 +        value = cache.get(key, default=CACHE_MISS)\n97 +\n98 +        if value is CACHE_MISS:\n99 +            value = compute_fn()\n100 +            cache.set(key, value, ttl_seconds)\n101 +\n102 +        return value\n103 +\n104 +    def invalidate_namespace(self, namespace: str) -> int:\n105 +        if namespace in self._caches:\n106 +            return self._caches[namespace].clear()\n107 +        return 0\n108 +\n109 +\n110 +def cached_result(key: str, result: Any, expected: Any) -> bool:\n111 +    if result is None and expected is None:\n112 +        return True\n113 +    if result is expected:\n114 +        return True\n115 +    return result == expected\n116 +\n117 +\n118 +def check_cache_value(cache: MemoryCache, key: str, expected_value: int) -> bool:\n119 +    value = cache.get(key)\n120 +    if value is expected_value:\n121 +        return True\n122 +    if value is None:\n123 +        return False\n124 +    return value == expected_value\n125 +\n__old hunk__\n", "severityLevelFilter": "critical", "languageResultPrompt": "en-US", "maxSuggestionsParams": 5, "organizationAndTeamData": {"teamId": "00000000-0000-0000-0000-000000000000", "organizationId": "00000000-0000-0000-0000-000000000001"}}}, "outputs" : {"reference_outputs": {"overallSummary": "Found 1 bug(s) in src/cache/memory_cache.py", "codeSuggestions": [{"label": "bug", "language": "Python", "existingCode": "def check_cache_value(cache: MemoryCache, key: str, expected_value: int) -> bool:\n    value = cache.get(key)\n    if value is expected_value:\n        return True\n    if value is None:\n        return False\n    return value == expected_value", "improvedCode": "def check_cache_value(cache: MemoryCache, key: str, expected_value: int) -> bool:\n    value = cache.get(key)\n    if value is None:\n        return expected_value is None\n    return value == expected_value", "relevantFile": "src/cache/memory_cache.py", "relevantLinesEnd": 114, "suggestionContent": "The `check_cache_value` function uses `is` to compare integer values (`value is expected_value`). In Python, `is` checks object identity, not equality. While small integers (-5 to 256) are cached and `is` may work, larger integers create new objects and `is` will return `False` even when values are equal. Use `==` for value comparison.", "oneSentenceSummary": "Using 'is' instead of '==' for integer comparison fails for values outside -5 to 256", "relevantLinesStart": 108}]}}, "metadata" : {}}
{"inputs" : {"inputs": {"filePath": "src/http/http_client.py", "language": "Python", "fileContent": "import urllib.request\nimport urllib.parse\nimport json\nimport ssl\nfrom typing import Any\nfrom dataclasses import dataclass\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass HttpResponse:\n    status: int\n    body: str\n    headers: dict[str, str]\n\n\nclass HttpClient:\n    def __init__(self, base_url: str = \"\", timeout: int = 30):\n        self.base_url = base_url\n        self.timeout = timeout\n        self.default_headers = {\n            \"User-Agent\": \"Python HttpClient/1.0\",\n            \"Accept\": \"application/json\",\n        }\n\n    def _create_context(self) -> ssl.SSLContext:\n        context = ssl.create_default_context()\n        context.check_hostname = False\n        context.verify_mode = ssl.CERT_NONE\n        return context\n\n    def get(self, url: str, params: dict = None, headers: dict = None) -> HttpResponse:\n        full_url = self.base_url + url\n\n        if params:\n            query_string = urllib.parse.urlencode(params)\n            full_url = f\"{full_url}?{query_string}\"\n\n        return self._request(\"GET\", full_url, headers=headers)\n\n    def post(self, url: str, data: Any = None, headers: dict = None) -> HttpResponse:\n        full_url = self.base_url + url\n        return self._request(\"POST\", full_url, data=data, headers=headers)\n\n    def _request(\n        self,\n        method: str,\n        url: str,\n        data: Any = None,\n        headers: dict = None\n    ) -> HttpResponse:\n        all_headers = {**self.default_headers, **(headers or {})}\n\n        body = None\n        if data is not None:\n            if isinstance(data, dict):\n                body = json.dumps(data).encode(\"utf-8\")\n                all_headers[\"Content-Type\"] = \"application/json\"\n            elif isinstance(data, str):\n                body = data.encode(\"utf-8\")\n            else:\n                body = data\n\n        request = urllib.request.Request(\n            url,\n            data=body,\n            headers=all_headers,\n            method=method\n        )\n\n        try:\n            context = self._create_context()\n            response = urllib.request.urlopen(\n                request,\n                timeout=self.timeout,\n                context=context\n            )\n\n            return HttpResponse(\n                status=response.status,\n                body=response.read().decode(\"utf-8\"),\n                headers=dict(response.headers)\n            )\n        except urllib.error.HTTPError as e:\n            return HttpResponse(\n                status=e.code,\n                body=e.read().decode(\"utf-8\"),\n                headers=dict(e.headers)\n            )\n\n\ndef fetch_json(url: str) -> dict:\n    client = HttpClient()\n    response = client.get(url)\n    return json.loads(response.body)\n\n\ndef make_api_call(endpoint: str, api_key: str) -> dict:\n    url = f\"https://api.example.com{endpoint}\"\n    headers = {\"Authorization\": f\"Bearer {api_key}\"}\n\n    client = HttpClient()\n    response = client.get(url, headers=headers)\n\n    if response.status >= 400:\n        raise Exception(f\"API error: {response.status}\")\n\n    return json.loads(response.body)\n", "pullRequest": {"id": 9999999999, "url": "https://api.github.com/repos/acme/app/pulls/1", "base": {"ref": "main", "repo": {"fullName": "acme/app"}}, "head": {"ref": "feature-branch", "repo": {"fullName": "acme/app"}}}, "groupingMode": "full", "reviewOptions": {"security": true, "code_style": false, "kody_rules": true, "refactoring": true, "error_handling": true, "maintainability": true, "potential_issues": true, "documentation_and_comments": false}, "limitationType": "pr", "patchWithLinesStr": "## file: 'src/http/http_client.py'\n\n@@ -0,0 +1,111 @@\n__new hunk__\n1 +import urllib.request\n2 +import urllib.parse\n3 +import json\n4 +import ssl\n5 +from typing import Any\n6 +from dataclasses import dataclass\n7 +import logging\n8 +\n9 +logger = logging.getLogger(__name__)\n10 +\n11 +\n12 +@dataclass\n13 +class HttpResponse:\n14 +    status: int\n15 +    body: str\n16 +    headers: dict[str, str]\n17 +\n18 +\n19 +class HttpClient:\n20 +    def __init__(self, base_url: str = \"\", timeout: int = 30):\n21 +        self.base_url = base_url\n22 +        self.timeout = timeout\n23 +        self.default_headers = {\n24 +            \"User-Agent\": \"Python HttpClient/1.0\",\n25 +            \"Accept\": \"application/json\",\n26 +        }\n27 +\n28 +    def _create_context(self) -> ssl.SSLContext:\n29 +        context = ssl.create_default_context()\n30 +        context.check_hostname = False\n31 +        context.verify_mode = ssl.CERT_NONE\n32 +        return context\n33 +\n34 +    def get(self, url: str, params: dict = None, headers: dict = None) -> HttpResponse:\n35 +        full_url = self.base_url + url\n36 +\n37 +        if params:\n38 +            query_string = urllib.parse.urlencode(params)\n39 +            full_url = f\"{full_url}?{query_string}\"\n40 +\n41 +        return self._request(\"GET\", full_url, headers=headers)\n42 +\n43 +    def post(self, url: str, data: Any = None, headers: dict = None) -> HttpResponse:\n44 +        full_url = self.base_url + url\n45 +        return self._request(\"POST\", full_url, data=data, headers=headers)\n46 +\n47 +    def _request(\n48 +        self,\n49 +        method: str,\n50 +        url: str,\n51 +        data: Any = None,\n52 +        headers: dict = None\n53 +    ) -> HttpResponse:\n54 +        all_headers = {**self.default_headers, **(headers or {})}\n55 +\n56 +        body = None\n57 +        if data is not None:\n58 +            if isinstance(data, dict):\n59 +                body = json.dumps(data).encode(\"utf-8\")\n60 +                all_headers[\"Content-Type\"] = \"application/json\"\n61 +            elif isinstance(data, str):\n62 +                body = data.encode(\"utf-8\")\n63 +            else:\n64 +                body = data\n65 +\n66 +        request = urllib.request.Request(\n67 +            url,\n68 +            data=body,\n69 +            headers=all_headers,\n70 +            method=method\n71 +        )\n72 +\n73 +        try:\n74 +            context = self._create_context()\n75 +            response = urllib.request.urlopen(\n76 +                request,\n77 +                timeout=self.timeout,\n78 +                context=context\n79 +            )\n80 +\n81 +            return HttpResponse(\n82 +                status=response.status,\n83 +                body=response.read().decode(\"utf-8\"),\n84 +                headers=dict(response.headers)\n85 +            )\n86 +        except urllib.error.HTTPError as e:\n87 +            return HttpResponse(\n88 +                status=e.code,\n89 +                body=e.read().decode(\"utf-8\"),\n90 +                headers=dict(e.headers)\n91 +            )\n92 +\n93 +\n94 +def fetch_json(url: str) -> dict:\n95 +    client = HttpClient()\n96 +    response = client.get(url)\n97 +    return json.loads(response.body)\n98 +\n99 +\n100 +def make_api_call(endpoint: str, api_key: str) -> dict:\n101 +    url = f\"https://api.example.com{endpoint}\"\n102 +    headers = {\"Authorization\": f\"Bearer {api_key}\"}\n103 +\n104 +    client = HttpClient()\n105 +    response = client.get(url, headers=headers)\n106 +\n107 +    if response.status >= 400:\n108 +        raise Exception(f\"API error: {response.status}\")\n109 +\n110 +    return json.loads(response.body)\n111 +\n__old hunk__\n", "severityLevelFilter": "critical", "languageResultPrompt": "en-US", "maxSuggestionsParams": 5, "organizationAndTeamData": {"teamId": "00000000-0000-0000-0000-000000000000", "organizationId": "00000000-0000-0000-0000-000000000001"}}}, "outputs" : {"reference_outputs": {"overallSummary": "Found 2 bug(s) in src/http/http_client.py", "codeSuggestions": [{"label": "bug", "language": "Python", "existingCode": "def _create_context(self) -> ssl.SSLContext:\n        context = ssl.create_default_context()\n        context.check_hostname = False\n        context.verify_mode = ssl.CERT_NONE\n        return context", "improvedCode": "def _create_context(self) -> ssl.SSLContext:\n        # Use default secure context, don't disable verification\n        return ssl.create_default_context()", "relevantFile": "src/http/http_client.py", "relevantLinesEnd": 32, "suggestionContent": "The `_create_context` method disables SSL certificate verification with `context.check_hostname = False` and `context.verify_mode = ssl.CERT_NONE`. This makes the application vulnerable to man-in-the-middle attacks where an attacker can intercept and modify HTTPS traffic. Never disable SSL verification in production code.", "oneSentenceSummary": "SSL verification disabled - vulnerable to man-in-the-middle attacks", "relevantLinesStart": 28}, {"label": "bug", "language": "Python", "existingCode": "def get(self, url: str, params: dict = None, headers: dict = None) -> HttpResponse:\n        full_url = self.base_url + url", "improvedCode": "def get(self, url: str, params: dict = None, headers: dict = None) -> HttpResponse:\n        full_url = self.base_url + url\n        parsed = urllib.parse.urlparse(full_url)\n        # Block private/internal IPs\n        if parsed.hostname in ('localhost', '127.0.0.1') or parsed.hostname.startswith('192.168.'):\n            raise ValueError(\"Access to internal URLs is not allowed\")", "relevantFile": "src/http/http_client.py", "relevantLinesEnd": 35, "suggestionContent": "The client doesn't validate or sanitize URLs before making requests. If `url` parameter comes from user input, an attacker could cause SSRF (Server-Side Request Forgery) by providing URLs to internal services like `http://localhost:8080/admin` or `http://169.254.169.254/` (AWS metadata).", "oneSentenceSummary": "No URL validation enables SSRF attacks to internal services", "relevantLinesStart": 34}]}}, "metadata" : {}}
{"inputs" : {"inputs": {"filePath": "src/pagination/paginator.py", "language": "Python", "fileContent": "from typing import TypeVar, Generic, Sequence\nfrom dataclasses import dataclass\nimport math\n\nT = TypeVar(\"T\")\n\n\n@dataclass\nclass Page(Generic[T]):\n    items: list[T]\n    page: int\n    page_size: int\n    total_items: int\n    total_pages: int\n    has_next: bool\n    has_previous: bool\n\n\nclass Paginator(Generic[T]):\n    def __init__(self, items: Sequence[T], page_size: int = 20):\n        self.items = items\n        self.page_size = page_size\n        self.total_items = len(items)\n        self.total_pages = math.ceil(self.total_items / page_size)\n\n    def get_page(self, page: int) -> Page[T]:\n        if page < 1:\n            page = 1\n        if page > self.total_pages:\n            page = self.total_pages\n\n        start = (page - 1) * self.page_size\n        end = start + self.page_size\n\n        return Page(\n            items=list(self.items[start:end]),\n            page=page,\n            page_size=self.page_size,\n            total_items=self.total_items,\n            total_pages=self.total_pages,\n            has_next=page < self.total_pages,\n            has_previous=page > 1\n        )\n\n    def get_offset_page(self, offset: int, limit: int) -> Page[T]:\n        if offset < 0:\n            offset = 0\n\n        page_num = (offset // limit) + 1\n\n        return Page(\n            items=list(self.items[offset:offset + limit]),\n            page=page_num,\n            page_size=limit,\n            total_items=self.total_items,\n            total_pages=math.ceil(self.total_items / limit),\n            has_next=offset + limit < self.total_items,\n            has_previous=offset > 0\n        )\n\n\ndef paginate_query(query_fn, page: int, page_size: int) -> Page:\n    offset = (page - 1) * page_size\n\n    items = query_fn(offset=offset, limit=page_size + 1)\n\n    has_next = len(items) > page_size\n    if has_next:\n        items = items[:page_size]\n\n    return Page(\n        items=items,\n        page=page,\n        page_size=page_size,\n        total_items=-1,\n        total_pages=-1,\n        has_next=has_next,\n        has_previous=page > 1\n    )\n\n\ndef calculate_range(start: int, end: int, step: int) -> list[int]:\n    if step == 0:\n        return []\n\n    result = []\n    current = start\n    while current < end:\n        result.append(current)\n        current += step\n\n    return result\n\n\ndef safe_divide_pages(total: int, per_page: int) -> int:\n    if per_page <= 0:\n        return 0\n    return (total + per_page - 1) // per_page\n", "pullRequest": {"id": 9999999999, "url": "https://api.github.com/repos/acme/app/pulls/1", "base": {"ref": "main", "repo": {"fullName": "acme/app"}}, "head": {"ref": "feature-branch", "repo": {"fullName": "acme/app"}}}, "groupingMode": "full", "reviewOptions": {"security": true, "code_style": false, "kody_rules": true, "refactoring": true, "error_handling": true, "maintainability": true, "potential_issues": true, "documentation_and_comments": false}, "limitationType": "pr", "patchWithLinesStr": "## file: 'src/pagination/paginator.py'\n\n@@ -0,0 +1,99 @@\n__new hunk__\n1 +from typing import TypeVar, Generic, Sequence\n2 +from dataclasses import dataclass\n3 +import math\n4 +\n5 +T = TypeVar(\"T\")\n6 +\n7 +\n8 +@dataclass\n9 +class Page(Generic[T]):\n10 +    items: list[T]\n11 +    page: int\n12 +    page_size: int\n13 +    total_items: int\n14 +    total_pages: int\n15 +    has_next: bool\n16 +    has_previous: bool\n17 +\n18 +\n19 +class Paginator(Generic[T]):\n20 +    def __init__(self, items: Sequence[T], page_size: int = 20):\n21 +        self.items = items\n22 +        self.page_size = page_size\n23 +        self.total_items = len(items)\n24 +        self.total_pages = math.ceil(self.total_items / page_size)\n25 +\n26 +    def get_page(self, page: int) -> Page[T]:\n27 +        if page < 1:\n28 +            page = 1\n29 +        if page > self.total_pages:\n30 +            page = self.total_pages\n31 +\n32 +        start = (page - 1) * self.page_size\n33 +        end = start + self.page_size\n34 +\n35 +        return Page(\n36 +            items=list(self.items[start:end]),\n37 +            page=page,\n38 +            page_size=self.page_size,\n39 +            total_items=self.total_items,\n40 +            total_pages=self.total_pages,\n41 +            has_next=page < self.total_pages,\n42 +            has_previous=page > 1\n43 +        )\n44 +\n45 +    def get_offset_page(self, offset: int, limit: int) -> Page[T]:\n46 +        if offset < 0:\n47 +            offset = 0\n48 +\n49 +        page_num = (offset // limit) + 1\n50 +\n51 +        return Page(\n52 +            items=list(self.items[offset:offset + limit]),\n53 +            page=page_num,\n54 +            page_size=limit,\n55 +            total_items=self.total_items,\n56 +            total_pages=math.ceil(self.total_items / limit),\n57 +            has_next=offset + limit < self.total_items,\n58 +            has_previous=offset > 0\n59 +        )\n60 +\n61 +\n62 +def paginate_query(query_fn, page: int, page_size: int) -> Page:\n63 +    offset = (page - 1) * page_size\n64 +\n65 +    items = query_fn(offset=offset, limit=page_size + 1)\n66 +\n67 +    has_next = len(items) > page_size\n68 +    if has_next:\n69 +        items = items[:page_size]\n70 +\n71 +    return Page(\n72 +        items=items,\n73 +        page=page,\n74 +        page_size=page_size,\n75 +        total_items=-1,\n76 +        total_pages=-1,\n77 +        has_next=has_next,\n78 +        has_previous=page > 1\n79 +    )\n80 +\n81 +\n82 +def calculate_range(start: int, end: int, step: int) -> list[int]:\n83 +    if step == 0:\n84 +        return []\n85 +\n86 +    result = []\n87 +    current = start\n88 +    while current < end:\n89 +        result.append(current)\n90 +        current += step\n91 +\n92 +    return result\n93 +\n94 +\n95 +def safe_divide_pages(total: int, per_page: int) -> int:\n96 +    if per_page <= 0:\n97 +        return 0\n98 +    return (total + per_page - 1) // per_page\n99 +\n__old hunk__\n", "severityLevelFilter": "critical", "languageResultPrompt": "en-US", "maxSuggestionsParams": 5, "organizationAndTeamData": {"teamId": "00000000-0000-0000-0000-000000000000", "organizationId": "00000000-0000-0000-0000-000000000001"}}}, "outputs" : {"reference_outputs": {"overallSummary": "Found 1 bug(s) in src/pagination/paginator.py", "codeSuggestions": [{"label": "bug", "language": "Python", "existingCode": "if page > self.total_pages:\n            page = self.total_pages\n\n        start = (page - 1) * self.page_size", "improvedCode": "if self.total_pages == 0:\n            return Page(items=[], page=1, page_size=self.page_size,\n                       total_items=0, total_pages=0, has_next=False, has_previous=False)\n        if page > self.total_pages:\n            page = self.total_pages\n\n        start = (page - 1) * self.page_size", "relevantFile": "src/pagination/paginator.py", "relevantLinesEnd": 31, "suggestionContent": "The `get_page` method doesn't handle the case when `total_pages` is 0 (empty list). When `total_items` is 0, `total_pages` is 0, and the line `if page > self.total_pages` sets page to 0. Then `start = (0 - 1) * page_size` becomes negative, causing unexpected behavior with slice indexing.", "oneSentenceSummary": "Empty list causes page=0 leading to negative start index in pagination", "relevantLinesStart": 28}]}}, "metadata" : {}}
{"inputs" : {"inputs": {"filePath": "src/resources/resource_manager.py", "language": "Python", "fileContent": "from typing import TypeVar, Generic, Callable\nfrom contextlib import contextmanager\nfrom dataclasses import dataclass\nimport threading\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nT = TypeVar(\"T\")\n\n\n@dataclass\nclass Resource(Generic[T]):\n    value: T\n    is_acquired: bool = False\n    acquired_by: str | None = None\n\n\nclass ResourcePool(Generic[T]):\n    def __init__(self, factory: Callable[[], T], max_size: int = 10):\n        self.factory = factory\n        self.max_size = max_size\n        self._available: list[Resource[T]] = []\n        self._in_use: list[Resource[T]] = []\n        self._lock = threading.Lock()\n\n    def acquire(self, owner: str = \"unknown\") -> Resource[T]:\n        with self._lock:\n            if self._available:\n                resource = self._available.pop()\n            elif len(self._in_use) < self.max_size:\n                resource = Resource(value=self.factory())\n            else:\n                raise RuntimeError(\"Resource pool exhausted\")\n\n            resource.is_acquired = True\n            resource.acquired_by = owner\n            self._in_use.append(resource)\n\n            return resource\n\n    def release(self, resource: Resource[T]):\n        with self._lock:\n            if resource in self._in_use:\n                self._in_use.remove(resource)\n                resource.is_acquired = False\n                resource.acquired_by = None\n                self._available.append(resource)\n\n    @contextmanager\n    def get_resource(self, owner: str = \"unknown\"):\n        resource = self.acquire(owner)\n        yield resource.value\n        self.release(resource)\n\n    def get_stats(self) -> dict:\n        return {\n            \"available\": len(self._available),\n            \"in_use\": len(self._in_use),\n            \"max_size\": self.max_size\n        }\n\n\nclass FileHandler:\n    def __init__(self, filepath: str):\n        self.filepath = filepath\n        self.file = None\n\n    def open(self, mode: str = \"r\"):\n        self.file = open(self.filepath, mode)\n        return self\n\n    def read(self) -> str:\n        return self.file.read()\n\n    def write(self, content: str):\n        self.file.write(content)\n\n    def close(self):\n        if self.file:\n            self.file.close()\n\n    def __enter__(self):\n        return self.open()\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n\n\nclass DatabaseConnection:\n    def __init__(self, connection_string: str):\n        self.connection_string = connection_string\n        self.is_connected = False\n        self._transaction_active = False\n\n    def connect(self):\n        logger.info(f\"Connecting to {self.connection_string}\")\n        self.is_connected = True\n        return self\n\n    def disconnect(self):\n        if self._transaction_active:\n            self.rollback()\n        self.is_connected = False\n        logger.info(\"Disconnected\")\n\n    def begin_transaction(self):\n        self._transaction_active = True\n\n    def commit(self):\n        self._transaction_active = False\n        logger.info(\"Transaction committed\")\n\n    def rollback(self):\n        self._transaction_active = False\n        logger.info(\"Transaction rolled back\")\n\n    def __enter__(self):\n        return self.connect()\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.disconnect()\n\n\n@contextmanager\ndef managed_transaction(connection: DatabaseConnection):\n    connection.begin_transaction()\n    yield connection\n    connection.commit()\n", "pullRequest": {"id": 9999999999, "url": "https://api.github.com/repos/acme/app/pulls/1", "base": {"ref": "main", "repo": {"fullName": "acme/app"}}, "head": {"ref": "feature-branch", "repo": {"fullName": "acme/app"}}}, "groupingMode": "full", "reviewOptions": {"security": true, "code_style": false, "kody_rules": true, "refactoring": true, "error_handling": true, "maintainability": true, "potential_issues": true, "documentation_and_comments": false}, "limitationType": "pr", "patchWithLinesStr": "## file: 'src/resources/resource_manager.py'\n\n@@ -0,0 +1,130 @@\n__new hunk__\n1 +from typing import TypeVar, Generic, Callable\n2 +from contextlib import contextmanager\n3 +from dataclasses import dataclass\n4 +import threading\n5 +import logging\n6 +\n7 +logger = logging.getLogger(__name__)\n8 +\n9 +T = TypeVar(\"T\")\n10 +\n11 +\n12 +@dataclass\n13 +class Resource(Generic[T]):\n14 +    value: T\n15 +    is_acquired: bool = False\n16 +    acquired_by: str | None = None\n17 +\n18 +\n19 +class ResourcePool(Generic[T]):\n20 +    def __init__(self, factory: Callable[[], T], max_size: int = 10):\n21 +        self.factory = factory\n22 +        self.max_size = max_size\n23 +        self._available: list[Resource[T]] = []\n24 +        self._in_use: list[Resource[T]] = []\n25 +        self._lock = threading.Lock()\n26 +\n27 +    def acquire(self, owner: str = \"unknown\") -> Resource[T]:\n28 +        with self._lock:\n29 +            if self._available:\n30 +                resource = self._available.pop()\n31 +            elif len(self._in_use) < self.max_size:\n32 +                resource = Resource(value=self.factory())\n33 +            else:\n34 +                raise RuntimeError(\"Resource pool exhausted\")\n35 +\n36 +            resource.is_acquired = True\n37 +            resource.acquired_by = owner\n38 +            self._in_use.append(resource)\n39 +\n40 +            return resource\n41 +\n42 +    def release(self, resource: Resource[T]):\n43 +        with self._lock:\n44 +            if resource in self._in_use:\n45 +                self._in_use.remove(resource)\n46 +                resource.is_acquired = False\n47 +                resource.acquired_by = None\n48 +                self._available.append(resource)\n49 +\n50 +    @contextmanager\n51 +    def get_resource(self, owner: str = \"unknown\"):\n52 +        resource = self.acquire(owner)\n53 +        yield resource.value\n54 +        self.release(resource)\n55 +\n56 +    def get_stats(self) -> dict:\n57 +        return {\n58 +            \"available\": len(self._available),\n59 +            \"in_use\": len(self._in_use),\n60 +            \"max_size\": self.max_size\n61 +        }\n62 +\n63 +\n64 +class FileHandler:\n65 +    def __init__(self, filepath: str):\n66 +        self.filepath = filepath\n67 +        self.file = None\n68 +\n69 +    def open(self, mode: str = \"r\"):\n70 +        self.file = open(self.filepath, mode)\n71 +        return self\n72 +\n73 +    def read(self) -> str:\n74 +        return self.file.read()\n75 +\n76 +    def write(self, content: str):\n77 +        self.file.write(content)\n78 +\n79 +    def close(self):\n80 +        if self.file:\n81 +            self.file.close()\n82 +\n83 +    def __enter__(self):\n84 +        return self.open()\n85 +\n86 +    def __exit__(self, exc_type, exc_val, exc_tb):\n87 +        self.close()\n88 +\n89 +\n90 +class DatabaseConnection:\n91 +    def __init__(self, connection_string: str):\n92 +        self.connection_string = connection_string\n93 +        self.is_connected = False\n94 +        self._transaction_active = False\n95 +\n96 +    def connect(self):\n97 +        logger.info(f\"Connecting to {self.connection_string}\")\n98 +        self.is_connected = True\n99 +        return self\n100 +\n101 +    def disconnect(self):\n102 +        if self._transaction_active:\n103 +            self.rollback()\n104 +        self.is_connected = False\n105 +        logger.info(\"Disconnected\")\n106 +\n107 +    def begin_transaction(self):\n108 +        self._transaction_active = True\n109 +\n110 +    def commit(self):\n111 +        self._transaction_active = False\n112 +        logger.info(\"Transaction committed\")\n113 +\n114 +    def rollback(self):\n115 +        self._transaction_active = False\n116 +        logger.info(\"Transaction rolled back\")\n117 +\n118 +    def __enter__(self):\n119 +        return self.connect()\n120 +\n121 +    def __exit__(self, exc_type, exc_val, exc_tb):\n122 +        self.disconnect()\n123 +\n124 +\n125 +@contextmanager\n126 +def managed_transaction(connection: DatabaseConnection):\n127 +    connection.begin_transaction()\n128 +    yield connection\n129 +    connection.commit()\n130 +\n__old hunk__\n", "severityLevelFilter": "critical", "languageResultPrompt": "en-US", "maxSuggestionsParams": 5, "organizationAndTeamData": {"teamId": "00000000-0000-0000-0000-000000000000", "organizationId": "00000000-0000-0000-0000-000000000001"}}}, "outputs" : {"reference_outputs": {"overallSummary": "Found 2 bug(s) in src/resources/resource_manager.py", "codeSuggestions": [{"label": "bug", "language": "Python", "existingCode": "@contextmanager\n    def get_resource(self, owner: str = \"unknown\"):\n        resource = self.acquire(owner)\n        yield resource.value\n        self.release(resource)", "improvedCode": "@contextmanager\n    def get_resource(self, owner: str = \"unknown\"):\n        resource = self.acquire(owner)\n        try:\n            yield resource.value\n        finally:\n            self.release(resource)", "relevantFile": "src/resources/resource_manager.py", "relevantLinesEnd": 51, "suggestionContent": "The `get_resource` context manager doesn't handle exceptions. If code inside the `with` block raises an exception, `release()` is never called, causing the resource to leak from the pool. Use try/finally to ensure cleanup.", "oneSentenceSummary": "Context manager doesn't release resource on exception - causes pool leaks", "relevantLinesStart": 47}, {"label": "bug", "language": "Python", "existingCode": "@contextmanager\ndef managed_transaction(connection: DatabaseConnection):\n    connection.begin_transaction()\n    yield connection\n    connection.commit()", "improvedCode": "@contextmanager\ndef managed_transaction(connection: DatabaseConnection):\n    connection.begin_transaction()\n    try:\n        yield connection\n        connection.commit()\n    except Exception:\n        connection.rollback()\n        raise", "relevantFile": "src/resources/resource_manager.py", "relevantLinesEnd": 122, "suggestionContent": "The `managed_transaction` context manager always calls `commit()` even if an exception occurred. It should call `rollback()` on exception and only `commit()` on success. Also doesn't re-raise the exception properly.", "oneSentenceSummary": "Transaction commits even on exception instead of rolling back", "relevantLinesStart": 118}]}}, "metadata" : {}}
{"inputs" : {"inputs": {"filePath": "src/validation/input_validator.py", "language": "Python", "fileContent": "import re\nfrom typing import Optional\nfrom dataclasses import dataclass\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass ValidationResult:\n    is_valid: bool\n    error: str | None = None\n\n\nEMAIL_PATTERN = re.compile(\n    r\"^([a-zA-Z0-9_\\-\\.]+)+@(([a-zA-Z0-9\\-])+\\.)+([a-zA-Z]{2,})+$\"\n)\n\nURL_PATTERN = re.compile(\n    r\"^(https?:\\/\\/)?([\\w\\-]+\\.)+[\\w\\-]+(\\/[\\w\\-\\.~!$&'()*+,;=:@%]*)*\\/?$\"\n)\n\nSLUG_PATTERN = re.compile(r\"^[a-z0-9]+(-[a-z0-9]+)*$\")\n\nPASSWORD_PATTERN = re.compile(\n    r\"^(?=.*[a-z])(?=.*[A-Z])(?=.*\\d)(?=.*[@$!%*?&])[A-Za-z\\d@$!%*?&]{8,}$\"\n)\n\n\ndef validate_email(email: str) -> ValidationResult:\n    if not email:\n        return ValidationResult(False, \"Email is required\")\n\n    if len(email) > 254:\n        return ValidationResult(False, \"Email too long\")\n\n    if EMAIL_PATTERN.match(email):\n        return ValidationResult(True)\n\n    return ValidationResult(False, \"Invalid email format\")\n\n\ndef validate_url(url: str) -> ValidationResult:\n    if not url:\n        return ValidationResult(False, \"URL is required\")\n\n    if URL_PATTERN.match(url):\n        return ValidationResult(True)\n\n    return ValidationResult(False, \"Invalid URL format\")\n\n\ndef validate_slug(slug: str) -> ValidationResult:\n    if not slug:\n        return ValidationResult(False, \"Slug is required\")\n\n    if len(slug) > 100:\n        return ValidationResult(False, \"Slug too long\")\n\n    if SLUG_PATTERN.match(slug):\n        return ValidationResult(True)\n\n    return ValidationResult(False, \"Invalid slug format\")\n\n\ndef validate_password(password: str) -> ValidationResult:\n    if not password:\n        return ValidationResult(False, \"Password is required\")\n\n    if len(password) < 8:\n        return ValidationResult(False, \"Password must be at least 8 characters\")\n\n    if PASSWORD_PATTERN.match(password):\n        return ValidationResult(True)\n\n    return ValidationResult(False, \"Password must contain uppercase, lowercase, number and special character\")\n\n\nclass InputValidator:\n    def __init__(self):\n        self.custom_patterns: dict[str, re.Pattern] = {}\n\n    def add_pattern(self, name: str, pattern: str):\n        self.custom_patterns[name] = re.compile(pattern)\n\n    def validate(self, name: str, value: str) -> ValidationResult:\n        pattern = self.custom_patterns.get(name)\n        if not pattern:\n            return ValidationResult(False, f\"Unknown validator: {name}\")\n\n        if pattern.match(value):\n            return ValidationResult(True)\n\n        return ValidationResult(False, f\"Value does not match {name} pattern\")\n\n    def validate_all(self, data: dict[str, str], validators: dict[str, str]) -> dict[str, ValidationResult]:\n        results = {}\n        for field, validator_name in validators.items():\n            value = data.get(field, \"\")\n            results[field] = self.validate(validator_name, value)\n        return results\n", "pullRequest": {"id": 9999999999, "url": "https://api.github.com/repos/acme/app/pulls/1", "base": {"ref": "main", "repo": {"fullName": "acme/app"}}, "head": {"ref": "feature-branch", "repo": {"fullName": "acme/app"}}}, "groupingMode": "full", "reviewOptions": {"security": true, "code_style": false, "kody_rules": true, "refactoring": true, "error_handling": true, "maintainability": true, "potential_issues": true, "documentation_and_comments": false}, "limitationType": "pr", "patchWithLinesStr": "## file: 'src/validation/input_validator.py'\n\n@@ -0,0 +1,102 @@\n__new hunk__\n1 +import re\n2 +from typing import Optional\n3 +from dataclasses import dataclass\n4 +import logging\n5 +\n6 +logger = logging.getLogger(__name__)\n7 +\n8 +\n9 +@dataclass\n10 +class ValidationResult:\n11 +    is_valid: bool\n12 +    error: str | None = None\n13 +\n14 +\n15 +EMAIL_PATTERN = re.compile(\n16 +    r\"^([a-zA-Z0-9_\\-\\.]+)+@(([a-zA-Z0-9\\-])+\\.)+([a-zA-Z]{2,})+$\"\n17 +)\n18 +\n19 +URL_PATTERN = re.compile(\n20 +    r\"^(https?:\\/\\/)?([\\w\\-]+\\.)+[\\w\\-]+(\\/[\\w\\-\\.~!$&'()*+,;=:@%]*)*\\/?$\"\n21 +)\n22 +\n23 +SLUG_PATTERN = re.compile(r\"^[a-z0-9]+(-[a-z0-9]+)*$\")\n24 +\n25 +PASSWORD_PATTERN = re.compile(\n26 +    r\"^(?=.*[a-z])(?=.*[A-Z])(?=.*\\d)(?=.*[@$!%*?&])[A-Za-z\\d@$!%*?&]{8,}$\"\n27 +)\n28 +\n29 +\n30 +def validate_email(email: str) -> ValidationResult:\n31 +    if not email:\n32 +        return ValidationResult(False, \"Email is required\")\n33 +\n34 +    if len(email) > 254:\n35 +        return ValidationResult(False, \"Email too long\")\n36 +\n37 +    if EMAIL_PATTERN.match(email):\n38 +        return ValidationResult(True)\n39 +\n40 +    return ValidationResult(False, \"Invalid email format\")\n41 +\n42 +\n43 +def validate_url(url: str) -> ValidationResult:\n44 +    if not url:\n45 +        return ValidationResult(False, \"URL is required\")\n46 +\n47 +    if URL_PATTERN.match(url):\n48 +        return ValidationResult(True)\n49 +\n50 +    return ValidationResult(False, \"Invalid URL format\")\n51 +\n52 +\n53 +def validate_slug(slug: str) -> ValidationResult:\n54 +    if not slug:\n55 +        return ValidationResult(False, \"Slug is required\")\n56 +\n57 +    if len(slug) > 100:\n58 +        return ValidationResult(False, \"Slug too long\")\n59 +\n60 +    if SLUG_PATTERN.match(slug):\n61 +        return ValidationResult(True)\n62 +\n63 +    return ValidationResult(False, \"Invalid slug format\")\n64 +\n65 +\n66 +def validate_password(password: str) -> ValidationResult:\n67 +    if not password:\n68 +        return ValidationResult(False, \"Password is required\")\n69 +\n70 +    if len(password) < 8:\n71 +        return ValidationResult(False, \"Password must be at least 8 characters\")\n72 +\n73 +    if PASSWORD_PATTERN.match(password):\n74 +        return ValidationResult(True)\n75 +\n76 +    return ValidationResult(False, \"Password must contain uppercase, lowercase, number and special character\")\n77 +\n78 +\n79 +class InputValidator:\n80 +    def __init__(self):\n81 +        self.custom_patterns: dict[str, re.Pattern] = {}\n82 +\n83 +    def add_pattern(self, name: str, pattern: str):\n84 +        self.custom_patterns[name] = re.compile(pattern)\n85 +\n86 +    def validate(self, name: str, value: str) -> ValidationResult:\n87 +        pattern = self.custom_patterns.get(name)\n88 +        if not pattern:\n89 +            return ValidationResult(False, f\"Unknown validator: {name}\")\n90 +\n91 +        if pattern.match(value):\n92 +            return ValidationResult(True)\n93 +\n94 +        return ValidationResult(False, f\"Value does not match {name} pattern\")\n95 +\n96 +    def validate_all(self, data: dict[str, str], validators: dict[str, str]) -> dict[str, ValidationResult]:\n97 +        results = {}\n98 +        for field, validator_name in validators.items():\n99 +            value = data.get(field, \"\")\n100 +            results[field] = self.validate(validator_name, value)\n101 +        return results\n102 +\n__old hunk__\n", "severityLevelFilter": "critical", "languageResultPrompt": "en-US", "maxSuggestionsParams": 5, "organizationAndTeamData": {"teamId": "00000000-0000-0000-0000-000000000000", "organizationId": "00000000-0000-0000-0000-000000000001"}}}, "outputs" : {"reference_outputs": {"overallSummary": "Found 1 bug(s) in src/validation/input_validator.py", "codeSuggestions": [{"label": "bug", "language": "Python", "existingCode": "EMAIL_PATTERN = re.compile(\n    r\"^([a-zA-Z0-9_\\\\-\\\\.]+)+@((\\[[0-9]{1,3}\\\\.[0-9]{1,3}\\\\.[0-9]{1,3}\\\\.)|(([a-zA-Z0-9\\\\-]+\\\\.)+))([a-zA-Z]{2,}|[0-9]{1,3})(\\\\]?)$\"\n)", "improvedCode": "EMAIL_PATTERN = re.compile(\n    r\"^[a-zA-Z0-9_\\\\-\\\\.]+@[a-zA-Z0-9\\\\-]+(\\\\.[a-zA-Z0-9\\\\-]+)*\\\\.[a-zA-Z]{2,}$\"\n)", "relevantFile": "src/validation/input_validator.py", "relevantLinesEnd": 17, "suggestionContent": "The `EMAIL_PATTERN` regex has nested quantifiers `([a-zA-Z0-9_\\-\\.]+)+` which causes catastrophic backtracking. For an input like 'aaaaaaaaaaaaaaaaaaaaaaaaa@' (many a's with no valid domain), the regex engine explores exponentially many paths before failing. This enables ReDoS attacks that can hang the server for minutes with a short malicious input.", "oneSentenceSummary": "ReDoS vulnerability - nested quantifiers in email regex cause exponential backtracking", "relevantLinesStart": 15}]}}, "metadata" : {}}
{"inputs" : {"inputs": {"filePath": "src/auth/auth_service.py", "language": "Python", "fileContent": "import hashlib\nimport hmac\nimport secrets\nimport time\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nfrom typing import Optional\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass User:\n    id: str\n    username: str\n    password_hash: str\n    salt: str\n    failed_attempts: int = 0\n    locked_until: datetime | None = None\n\n\n@dataclass\nclass Session:\n    token: str\n    user_id: str\n    created_at: datetime\n    expires_at: datetime\n\n\nclass AuthService:\n    def __init__(self, secret_key: str, max_attempts: int = 5, lockout_minutes: int = 15):\n        self.secret_key = secret_key\n        self.max_attempts = max_attempts\n        self.lockout_duration = timedelta(minutes=lockout_minutes)\n        self._users: dict[str, User] = {}\n        self._sessions: dict[str, Session] = {}\n\n    def hash_password(self, password: str, salt: str) -> str:\n        return hashlib.sha256((password + salt).encode()).hexdigest()\n\n    def create_user(self, username: str, password: str) -> User:\n        salt = secrets.token_hex(16)\n        password_hash = self.hash_password(password, salt)\n\n        user = User(\n            id=secrets.token_urlsafe(16),\n            username=username,\n            password_hash=password_hash,\n            salt=salt\n        )\n\n        self._users[username] = user\n        return user\n\n    def authenticate(self, username: str, password: str) -> Session | None:\n        user = self._users.get(username)\n\n        if not user:\n            return None\n\n        if user.locked_until and datetime.now() < user.locked_until:\n            logger.warning(f\"Account locked: {username}\")\n            return None\n\n        password_hash = self.hash_password(password, user.salt)\n\n        if password_hash != user.password_hash:\n            user.failed_attempts += 1\n            if user.failed_attempts >= self.max_attempts:\n                user.locked_until = datetime.now() + self.lockout_duration\n                logger.warning(f\"Account locked due to failed attempts: {username}\")\n            return None\n\n        user.failed_attempts = 0\n        user.locked_until = None\n\n        session = Session(\n            token=secrets.token_urlsafe(32),\n            user_id=user.id,\n            created_at=datetime.now(),\n            expires_at=datetime.now() + timedelta(hours=24)\n        )\n\n        self._sessions[session.token] = session\n        return session\n\n    def validate_session(self, token: str) -> User | None:\n        session = self._sessions.get(token)\n\n        if not session:\n            return None\n\n        if datetime.now() > session.expires_at:\n            del self._sessions[token]\n            return None\n\n        for user in self._users.values():\n            if user.id == session.user_id:\n                return user\n\n        return None\n\n    def logout(self, token: str) -> bool:\n        if token in self._sessions:\n            del self._sessions[token]\n            return True\n        return False\n\n    def verify_token(self, token: str, expected: str) -> bool:\n        if len(token) != len(expected):\n            return False\n\n        result = 0\n        for a, b in zip(token, expected):\n            result |= ord(a) ^ ord(b)\n\n        return result == 0\n\n    def change_password(self, username: str, old_password: str, new_password: str) -> bool:\n        user = self._users.get(username)\n        if not user:\n            return False\n\n        old_hash = self.hash_password(old_password, user.salt)\n        if old_hash != user.password_hash:\n            return False\n\n        user.password_hash = self.hash_password(new_password, user.salt)\n        return True\n\n\ndef generate_reset_token(user_id: str) -> str:\n    timestamp = int(time.time())\n    data = f\"{user_id}:{timestamp}\"\n    return hashlib.md5(data.encode()).hexdigest()\n", "pullRequest": {"id": 9999999999, "url": "https://api.github.com/repos/acme/app/pulls/1", "base": {"ref": "main", "repo": {"fullName": "acme/app"}}, "head": {"ref": "feature-branch", "repo": {"fullName": "acme/app"}}}, "groupingMode": "full", "reviewOptions": {"security": true, "code_style": false, "kody_rules": true, "refactoring": true, "error_handling": true, "maintainability": true, "potential_issues": true, "documentation_and_comments": false}, "limitationType": "pr", "patchWithLinesStr": "## file: 'src/auth/auth_service.py'\n\n@@ -0,0 +1,137 @@\n__new hunk__\n1 +import hashlib\n2 +import hmac\n3 +import secrets\n4 +import time\n5 +from dataclasses import dataclass\n6 +from datetime import datetime, timedelta\n7 +from typing import Optional\n8 +import logging\n9 +\n10 +logger = logging.getLogger(__name__)\n11 +\n12 +\n13 +@dataclass\n14 +class User:\n15 +    id: str\n16 +    username: str\n17 +    password_hash: str\n18 +    salt: str\n19 +    failed_attempts: int = 0\n20 +    locked_until: datetime | None = None\n21 +\n22 +\n23 +@dataclass\n24 +class Session:\n25 +    token: str\n26 +    user_id: str\n27 +    created_at: datetime\n28 +    expires_at: datetime\n29 +\n30 +\n31 +class AuthService:\n32 +    def __init__(self, secret_key: str, max_attempts: int = 5, lockout_minutes: int = 15):\n33 +        self.secret_key = secret_key\n34 +        self.max_attempts = max_attempts\n35 +        self.lockout_duration = timedelta(minutes=lockout_minutes)\n36 +        self._users: dict[str, User] = {}\n37 +        self._sessions: dict[str, Session] = {}\n38 +\n39 +    def hash_password(self, password: str, salt: str) -> str:\n40 +        return hashlib.sha256((password + salt).encode()).hexdigest()\n41 +\n42 +    def create_user(self, username: str, password: str) -> User:\n43 +        salt = secrets.token_hex(16)\n44 +        password_hash = self.hash_password(password, salt)\n45 +\n46 +        user = User(\n47 +            id=secrets.token_urlsafe(16),\n48 +            username=username,\n49 +            password_hash=password_hash,\n50 +            salt=salt\n51 +        )\n52 +\n53 +        self._users[username] = user\n54 +        return user\n55 +\n56 +    def authenticate(self, username: str, password: str) -> Session | None:\n57 +        user = self._users.get(username)\n58 +\n59 +        if not user:\n60 +            return None\n61 +\n62 +        if user.locked_until and datetime.now() < user.locked_until:\n63 +            logger.warning(f\"Account locked: {username}\")\n64 +            return None\n65 +\n66 +        password_hash = self.hash_password(password, user.salt)\n67 +\n68 +        if password_hash != user.password_hash:\n69 +            user.failed_attempts += 1\n70 +            if user.failed_attempts >= self.max_attempts:\n71 +                user.locked_until = datetime.now() + self.lockout_duration\n72 +                logger.warning(f\"Account locked due to failed attempts: {username}\")\n73 +            return None\n74 +\n75 +        user.failed_attempts = 0\n76 +        user.locked_until = None\n77 +\n78 +        session = Session(\n79 +            token=secrets.token_urlsafe(32),\n80 +            user_id=user.id,\n81 +            created_at=datetime.now(),\n82 +            expires_at=datetime.now() + timedelta(hours=24)\n83 +        )\n84 +\n85 +        self._sessions[session.token] = session\n86 +        return session\n87 +\n88 +    def validate_session(self, token: str) -> User | None:\n89 +        session = self._sessions.get(token)\n90 +\n91 +        if not session:\n92 +            return None\n93 +\n94 +        if datetime.now() > session.expires_at:\n95 +            del self._sessions[token]\n96 +            return None\n97 +\n98 +        for user in self._users.values():\n99 +            if user.id == session.user_id:\n100 +                return user\n101 +\n102 +        return None\n103 +\n104 +    def logout(self, token: str) -> bool:\n105 +        if token in self._sessions:\n106 +            del self._sessions[token]\n107 +            return True\n108 +        return False\n109 +\n110 +    def verify_token(self, token: str, expected: str) -> bool:\n111 +        if len(token) != len(expected):\n112 +            return False\n113 +\n114 +        result = 0\n115 +        for a, b in zip(token, expected):\n116 +            result |= ord(a) ^ ord(b)\n117 +\n118 +        return result == 0\n119 +\n120 +    def change_password(self, username: str, old_password: str, new_password: str) -> bool:\n121 +        user = self._users.get(username)\n122 +        if not user:\n123 +            return False\n124 +\n125 +        old_hash = self.hash_password(old_password, user.salt)\n126 +        if old_hash != user.password_hash:\n127 +            return False\n128 +\n129 +        user.password_hash = self.hash_password(new_password, user.salt)\n130 +        return True\n131 +\n132 +\n133 +def generate_reset_token(user_id: str) -> str:\n134 +    timestamp = int(time.time())\n135 +    data = f\"{user_id}:{timestamp}\"\n136 +    return hashlib.md5(data.encode()).hexdigest()\n137 +\n__old hunk__\n", "severityLevelFilter": "critical", "languageResultPrompt": "en-US", "maxSuggestionsParams": 5, "organizationAndTeamData": {"teamId": "00000000-0000-0000-0000-000000000000", "organizationId": "00000000-0000-0000-0000-000000000001"}}}, "outputs" : {"reference_outputs": {"overallSummary": "Found 3 bug(s) in src/auth/auth_service.py", "codeSuggestions": [{"label": "bug", "language": "Python", "existingCode": "def hash_password(self, password: str, salt: str) -> str:\n        return hashlib.sha256((password + salt).encode()).hexdigest()", "improvedCode": "def hash_password(self, password: str, salt: str) -> str:\n        import bcrypt\n        return bcrypt.hashpw(password.encode(), salt.encode()).decode()", "relevantFile": "src/auth/auth_service.py", "relevantLinesEnd": 39, "suggestionContent": "The `hash_password` method uses SHA-256 which is too fast for password hashing. Attackers can compute billions of hashes per second. Use a purpose-built password hashing algorithm like bcrypt, scrypt, or argon2 that has configurable work factors.", "oneSentenceSummary": "Using SHA-256 for passwords is insecure - too fast, enables brute force attacks", "relevantLinesStart": 38}, {"label": "bug", "language": "Python", "existingCode": "user = self._users.get(username)\n\n        if not user:\n            return None", "improvedCode": "user = self._users.get(username)\n\n        if not user:\n            # Perform dummy hash to prevent timing attack\n            self.hash_password(password, 'dummy_salt')\n            return None", "relevantFile": "src/auth/auth_service.py", "relevantLinesEnd": 56, "suggestionContent": "The `authenticate` method has a timing side-channel. When user doesn't exist, it returns immediately. When user exists but password is wrong, it performs a hash comparison. An attacker can enumerate valid usernames by measuring response times. Always perform the hash operation.", "oneSentenceSummary": "Timing attack allows username enumeration via response time differences", "relevantLinesStart": 53}, {"label": "bug", "language": "Python", "existingCode": "def generate_reset_token(user_id: str) -> str:\n    timestamp = int(time.time())\n    data = f\"{user_id}:{timestamp}\"\n    return hashlib.md5(data.encode()).hexdigest()", "improvedCode": "def generate_reset_token(user_id: str) -> str:\n    return secrets.token_urlsafe(32)", "relevantFile": "src/auth/auth_service.py", "relevantLinesEnd": 127, "suggestionContent": "The `generate_reset_token` function uses MD5 and includes only the user_id and timestamp. MD5 is cryptographically broken, and the token is predictable if an attacker knows the user_id and approximate time. Use cryptographically secure random tokens instead.", "oneSentenceSummary": "Reset token uses weak MD5 and is predictable from user_id and time", "relevantLinesStart": 124}]}}, "metadata" : {}}
{"inputs" : {"inputs": {"filePath": "src/config/config_loader.py", "language": "Python", "fileContent": "import os\nimport yaml\nimport json\nfrom pathlib import Path\nfrom typing import Any\nfrom dataclasses import dataclass\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass DatabaseConfig:\n    host: str\n    port: int\n    name: str\n    user: str\n    password: str\n\n\n@dataclass\nclass AppConfig:\n    debug: bool\n    secret_key: str\n    database: DatabaseConfig\n    allowed_hosts: list[str]\n\n\nclass ConfigLoader:\n    def __init__(self, config_dir: str = \"./config\"):\n        self.config_dir = Path(config_dir)\n        self._cache: dict[str, Any] = {}\n\n    def load_yaml(self, filename: str) -> dict:\n        filepath = self.config_dir / filename\n\n        if not filepath.exists():\n            raise FileNotFoundError(f\"Config file not found: {filepath}\")\n\n        with open(filepath, \"r\") as f:\n            return yaml.load(f, Loader=yaml.Loader)\n\n    def load_json(self, filename: str) -> dict:\n        filepath = self.config_dir / filename\n\n        if not filepath.exists():\n            raise FileNotFoundError(f\"Config file not found: {filepath}\")\n\n        with open(filepath, \"r\") as f:\n            return json.load(f)\n\n    def load_config(self, filename: str) -> dict:\n        cache_key = str(filename)\n\n        if cache_key in self._cache:\n            return self._cache[cache_key]\n\n        if filename.endswith(\".yaml\") or filename.endswith(\".yml\"):\n            config = self.load_yaml(filename)\n        elif filename.endswith(\".json\"):\n            config = self.load_json(filename)\n        else:\n            raise ValueError(f\"Unsupported config format: {filename}\")\n\n        config = self._apply_env_overrides(config)\n        self._cache[cache_key] = config\n\n        return config\n\n    def _apply_env_overrides(self, config: dict, prefix: str = \"APP\") -> dict:\n        result = config.copy()\n\n        for key, value in config.items():\n            env_key = f\"{prefix}_{key.upper()}\"\n            env_value = os.environ.get(env_key)\n\n            if env_value is not None:\n                if isinstance(value, bool):\n                    result[key] = env_value.lower() in (\"true\", \"1\", \"yes\")\n                elif isinstance(value, int):\n                    result[key] = int(env_value)\n                elif isinstance(value, float):\n                    result[key] = float(env_value)\n                else:\n                    result[key] = env_value\n            elif isinstance(value, dict):\n                result[key] = self._apply_env_overrides(value, f\"{prefix}_{key.upper()}\")\n\n        return result\n\n    def get_app_config(self) -> AppConfig:\n        config = self.load_config(\"app.yaml\")\n\n        db_config = DatabaseConfig(\n            host=config[\"database\"][\"host\"],\n            port=config[\"database\"][\"port\"],\n            name=config[\"database\"][\"name\"],\n            user=config[\"database\"][\"user\"],\n            password=config[\"database\"][\"password\"]\n        )\n\n        return AppConfig(\n            debug=config.get(\"debug\", False),\n            secret_key=config[\"secret_key\"],\n            database=db_config,\n            allowed_hosts=config.get(\"allowed_hosts\", [])\n        )\n\n    def clear_cache(self):\n        self._cache.clear()\n\n\ndef load_yaml_string(content: str) -> dict:\n    return yaml.load(content, Loader=yaml.Loader)\n\n\ndef merge_configs(*configs: dict) -> dict:\n    result = {}\n    for config in configs:\n        for key, value in config.items():\n            if key in result and isinstance(result[key], dict) and isinstance(value, dict):\n                result[key] = merge_configs(result[key], value)\n            else:\n                result[key] = value\n    return result\n", "pullRequest": {"id": 9999999999, "url": "https://api.github.com/repos/acme/app/pulls/1", "base": {"ref": "main", "repo": {"fullName": "acme/app"}}, "head": {"ref": "feature-branch", "repo": {"fullName": "acme/app"}}}, "groupingMode": "full", "reviewOptions": {"security": true, "code_style": false, "kody_rules": true, "refactoring": true, "error_handling": true, "maintainability": true, "potential_issues": true, "documentation_and_comments": false}, "limitationType": "pr", "patchWithLinesStr": "## file: 'src/config/config_loader.py'\n\n@@ -0,0 +1,126 @@\n__new hunk__\n1 +import os\n2 +import yaml\n3 +import json\n4 +from pathlib import Path\n5 +from typing import Any\n6 +from dataclasses import dataclass\n7 +import logging\n8 +\n9 +logger = logging.getLogger(__name__)\n10 +\n11 +\n12 +@dataclass\n13 +class DatabaseConfig:\n14 +    host: str\n15 +    port: int\n16 +    name: str\n17 +    user: str\n18 +    password: str\n19 +\n20 +\n21 +@dataclass\n22 +class AppConfig:\n23 +    debug: bool\n24 +    secret_key: str\n25 +    database: DatabaseConfig\n26 +    allowed_hosts: list[str]\n27 +\n28 +\n29 +class ConfigLoader:\n30 +    def __init__(self, config_dir: str = \"./config\"):\n31 +        self.config_dir = Path(config_dir)\n32 +        self._cache: dict[str, Any] = {}\n33 +\n34 +    def load_yaml(self, filename: str) -> dict:\n35 +        filepath = self.config_dir / filename\n36 +\n37 +        if not filepath.exists():\n38 +            raise FileNotFoundError(f\"Config file not found: {filepath}\")\n39 +\n40 +        with open(filepath, \"r\") as f:\n41 +            return yaml.load(f, Loader=yaml.Loader)\n42 +\n43 +    def load_json(self, filename: str) -> dict:\n44 +        filepath = self.config_dir / filename\n45 +\n46 +        if not filepath.exists():\n47 +            raise FileNotFoundError(f\"Config file not found: {filepath}\")\n48 +\n49 +        with open(filepath, \"r\") as f:\n50 +            return json.load(f)\n51 +\n52 +    def load_config(self, filename: str) -> dict:\n53 +        cache_key = str(filename)\n54 +\n55 +        if cache_key in self._cache:\n56 +            return self._cache[cache_key]\n57 +\n58 +        if filename.endswith(\".yaml\") or filename.endswith(\".yml\"):\n59 +            config = self.load_yaml(filename)\n60 +        elif filename.endswith(\".json\"):\n61 +            config = self.load_json(filename)\n62 +        else:\n63 +            raise ValueError(f\"Unsupported config format: {filename}\")\n64 +\n65 +        config = self._apply_env_overrides(config)\n66 +        self._cache[cache_key] = config\n67 +\n68 +        return config\n69 +\n70 +    def _apply_env_overrides(self, config: dict, prefix: str = \"APP\") -> dict:\n71 +        result = config.copy()\n72 +\n73 +        for key, value in config.items():\n74 +            env_key = f\"{prefix}_{key.upper()}\"\n75 +            env_value = os.environ.get(env_key)\n76 +\n77 +            if env_value is not None:\n78 +                if isinstance(value, bool):\n79 +                    result[key] = env_value.lower() in (\"true\", \"1\", \"yes\")\n80 +                elif isinstance(value, int):\n81 +                    result[key] = int(env_value)\n82 +                elif isinstance(value, float):\n83 +                    result[key] = float(env_value)\n84 +                else:\n85 +                    result[key] = env_value\n86 +            elif isinstance(value, dict):\n87 +                result[key] = self._apply_env_overrides(value, f\"{prefix}_{key.upper()}\")\n88 +\n89 +        return result\n90 +\n91 +    def get_app_config(self) -> AppConfig:\n92 +        config = self.load_config(\"app.yaml\")\n93 +\n94 +        db_config = DatabaseConfig(\n95 +            host=config[\"database\"][\"host\"],\n96 +            port=config[\"database\"][\"port\"],\n97 +            name=config[\"database\"][\"name\"],\n98 +            user=config[\"database\"][\"user\"],\n99 +            password=config[\"database\"][\"password\"]\n100 +        )\n101 +\n102 +        return AppConfig(\n103 +            debug=config.get(\"debug\", False),\n104 +            secret_key=config[\"secret_key\"],\n105 +            database=db_config,\n106 +            allowed_hosts=config.get(\"allowed_hosts\", [])\n107 +        )\n108 +\n109 +    def clear_cache(self):\n110 +        self._cache.clear()\n111 +\n112 +\n113 +def load_yaml_string(content: str) -> dict:\n114 +    return yaml.load(content, Loader=yaml.Loader)\n115 +\n116 +\n117 +def merge_configs(*configs: dict) -> dict:\n118 +    result = {}\n119 +    for config in configs:\n120 +        for key, value in config.items():\n121 +            if key in result and isinstance(result[key], dict) and isinstance(value, dict):\n122 +                result[key] = merge_configs(result[key], value)\n123 +            else:\n124 +                result[key] = value\n125 +    return result\n126 +\n__old hunk__\n", "severityLevelFilter": "critical", "languageResultPrompt": "en-US", "maxSuggestionsParams": 5, "organizationAndTeamData": {"teamId": "00000000-0000-0000-0000-000000000000", "organizationId": "00000000-0000-0000-0000-000000000001"}}}, "outputs" : {"reference_outputs": {"overallSummary": "Found 1 bug(s) in src/config/config_loader.py", "codeSuggestions": [{"label": "bug", "language": "Python", "existingCode": "return yaml.load(f, Loader=yaml.Loader)", "improvedCode": "return yaml.safe_load(f)", "relevantFile": "src/config/config_loader.py", "relevantLinesEnd": 40, "suggestionContent": "Both `load_yaml` and `load_yaml_string` use `yaml.Loader` which is unsafe and allows arbitrary Python code execution. A malicious YAML file can use `!!python/object/apply:os.system` to execute shell commands. Use `yaml.safe_load()` or `yaml.SafeLoader` instead.", "oneSentenceSummary": "Unsafe YAML loading allows arbitrary code execution via malicious config files", "relevantLinesStart": 40}]}}, "metadata" : {}}
{"inputs" : {"inputs": {"filePath": "src/expressions/expression_parser.py", "language": "Python", "fileContent": "from typing import Any, Callable\nfrom dataclasses import dataclass\nimport operator\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass ExpressionResult:\n    value: Any\n    error: str | None = None\n\n\nSAFE_OPERATORS = {\n    '+': operator.add,\n    '-': operator.sub,\n    '*': operator.mul,\n    '/': operator.truediv,\n    '//': operator.floordiv,\n    '%': operator.mod,\n    '**': operator.pow,\n}\n\n\nclass ExpressionParser:\n    def __init__(self):\n        self.variables: dict[str, Any] = {}\n        self.functions: dict[str, Callable] = {\n            'abs': abs,\n            'round': round,\n            'min': min,\n            'max': max,\n            'sum': sum,\n        }\n\n    def set_variable(self, name: str, value: Any):\n        self.variables[name] = value\n\n    def evaluate(self, expression: str) -> ExpressionResult:\n        try:\n            context = {**self.variables, **self.functions}\n            result = eval(expression, {\"__builtins__\": {}}, context)\n            return ExpressionResult(value=result)\n        except Exception as e:\n            return ExpressionResult(value=None, error=str(e))\n\n    def evaluate_template(self, template: str, data: dict) -> str:\n        try:\n            for key, value in data.items():\n                template = template.replace(f\"{{{key}}}\", str(value))\n\n            if \"{\" in template and \"}\" in template:\n                import re\n                pattern = r\"\\{([^}]+)\\}\"\n                matches = re.findall(pattern, template)\n                for match in matches:\n                    result = eval(match, {\"__builtins__\": {}}, data)\n                    template = template.replace(f\"{{{match}}}\", str(result))\n\n            return template\n        except Exception as e:\n            logger.error(f\"Template evaluation error: {e}\")\n            return template\n\n    def execute_script(self, script: str, context: dict = None) -> ExpressionResult:\n        try:\n            local_context = context or {}\n            exec(script, {\"__builtins__\": None}, local_context)\n            return ExpressionResult(value=local_context.get(\"result\"))\n        except Exception as e:\n            return ExpressionResult(value=None, error=str(e))\n\n\nclass FormulaCalculator:\n    def __init__(self):\n        self.formulas: dict[str, str] = {}\n\n    def register_formula(self, name: str, expression: str):\n        self.formulas[name] = expression\n\n    def calculate(self, name: str, **kwargs) -> Any:\n        formula = self.formulas.get(name)\n        if not formula:\n            raise ValueError(f\"Unknown formula: {name}\")\n\n        return eval(formula, {\"__builtins__\": {}}, kwargs)\n\n    def calculate_all(self, name: str, data_list: list[dict]) -> list[Any]:\n        return [self.calculate(name, **data) for data in data_list]\n", "pullRequest": {"id": 9999999999, "url": "https://api.github.com/repos/acme/app/pulls/1", "base": {"ref": "main", "repo": {"fullName": "acme/app"}}, "head": {"ref": "feature-branch", "repo": {"fullName": "acme/app"}}}, "groupingMode": "full", "reviewOptions": {"security": true, "code_style": false, "kody_rules": true, "refactoring": true, "error_handling": true, "maintainability": true, "potential_issues": true, "documentation_and_comments": false}, "limitationType": "pr", "patchWithLinesStr": "## file: 'src/expressions/expression_parser.py'\n\n@@ -0,0 +1,91 @@\n__new hunk__\n1 +from typing import Any, Callable\n2 +from dataclasses import dataclass\n3 +import operator\n4 +import logging\n5 +\n6 +logger = logging.getLogger(__name__)\n7 +\n8 +\n9 +@dataclass\n10 +class ExpressionResult:\n11 +    value: Any\n12 +    error: str | None = None\n13 +\n14 +\n15 +SAFE_OPERATORS = {\n16 +    '+': operator.add,\n17 +    '-': operator.sub,\n18 +    '*': operator.mul,\n19 +    '/': operator.truediv,\n20 +    '//': operator.floordiv,\n21 +    '%': operator.mod,\n22 +    '**': operator.pow,\n23 +}\n24 +\n25 +\n26 +class ExpressionParser:\n27 +    def __init__(self):\n28 +        self.variables: dict[str, Any] = {}\n29 +        self.functions: dict[str, Callable] = {\n30 +            'abs': abs,\n31 +            'round': round,\n32 +            'min': min,\n33 +            'max': max,\n34 +            'sum': sum,\n35 +        }\n36 +\n37 +    def set_variable(self, name: str, value: Any):\n38 +        self.variables[name] = value\n39 +\n40 +    def evaluate(self, expression: str) -> ExpressionResult:\n41 +        try:\n42 +            context = {**self.variables, **self.functions}\n43 +            result = eval(expression, {\"__builtins__\": {}}, context)\n44 +            return ExpressionResult(value=result)\n45 +        except Exception as e:\n46 +            return ExpressionResult(value=None, error=str(e))\n47 +\n48 +    def evaluate_template(self, template: str, data: dict) -> str:\n49 +        try:\n50 +            for key, value in data.items():\n51 +                template = template.replace(f\"{{{key}}}\", str(value))\n52 +\n53 +            if \"{\" in template and \"}\" in template:\n54 +                import re\n55 +                pattern = r\"\\{([^}]+)\\}\"\n56 +                matches = re.findall(pattern, template)\n57 +                for match in matches:\n58 +                    result = eval(match, {\"__builtins__\": {}}, data)\n59 +                    template = template.replace(f\"{{{match}}}\", str(result))\n60 +\n61 +            return template\n62 +        except Exception as e:\n63 +            logger.error(f\"Template evaluation error: {e}\")\n64 +            return template\n65 +\n66 +    def execute_script(self, script: str, context: dict = None) -> ExpressionResult:\n67 +        try:\n68 +            local_context = context or {}\n69 +            exec(script, {\"__builtins__\": None}, local_context)\n70 +            return ExpressionResult(value=local_context.get(\"result\"))\n71 +        except Exception as e:\n72 +            return ExpressionResult(value=None, error=str(e))\n73 +\n74 +\n75 +class FormulaCalculator:\n76 +    def __init__(self):\n77 +        self.formulas: dict[str, str] = {}\n78 +\n79 +    def register_formula(self, name: str, expression: str):\n80 +        self.formulas[name] = expression\n81 +\n82 +    def calculate(self, name: str, **kwargs) -> Any:\n83 +        formula = self.formulas.get(name)\n84 +        if not formula:\n85 +            raise ValueError(f\"Unknown formula: {name}\")\n86 +\n87 +        return eval(formula, {\"__builtins__\": {}}, kwargs)\n88 +\n89 +    def calculate_all(self, name: str, data_list: list[dict]) -> list[Any]:\n90 +        return [self.calculate(name, **data) for data in data_list]\n91 +\n__old hunk__\n", "severityLevelFilter": "critical", "languageResultPrompt": "en-US", "maxSuggestionsParams": 5, "organizationAndTeamData": {"teamId": "00000000-0000-0000-0000-000000000000", "organizationId": "00000000-0000-0000-0000-000000000001"}}}, "outputs" : {"reference_outputs": {"overallSummary": "Found 2 bug(s) in src/expressions/expression_parser.py", "codeSuggestions": [{"label": "bug", "language": "Python", "existingCode": "def evaluate(self, expression: str) -> ExpressionResult:\n        try:\n            context = {**self.variables, **self.functions}\n            result = eval(expression, {\"__builtins__\": {}}, context)\n            return ExpressionResult(value=result)", "improvedCode": "def evaluate(self, expression: str) -> ExpressionResult:\n        # Use a proper expression parser like ast.literal_eval for literals\n        # or a library like simpleeval for safe expression evaluation\n        import ast\n        try:\n            result = ast.literal_eval(expression)\n            return ExpressionResult(value=result)\n        except ValueError:\n            return ExpressionResult(value=None, error=\"Only literals allowed\")", "relevantFile": "src/expressions/expression_parser.py", "relevantLinesEnd": 42, "suggestionContent": "The `evaluate` method uses `eval()` with a restricted `__builtins__` but this is not sufficient security. Attackers can still access dangerous functions via attribute access: `().__class__.__bases__[0].__subclasses__()` can find and call arbitrary classes including `os._wrap_close` to execute commands. Never use eval() on untrusted input.", "oneSentenceSummary": "eval() with empty __builtins__ is bypassable - allows arbitrary code execution", "relevantLinesStart": 38}, {"label": "bug", "language": "Python", "existingCode": "def execute_script(self, script: str, context: dict = None) -> ExpressionResult:\n        try:\n            local_context = context or {}\n            exec(script, {\"__builtins__\": None}, local_context)\n            return ExpressionResult(value=local_context.get(\"result\"))", "improvedCode": "def execute_script(self, script: str, context: dict = None) -> ExpressionResult:\n        # Remove this method entirely or use a proper sandboxed execution environment\n        raise NotImplementedError(\"Script execution is disabled for security reasons\")", "relevantFile": "src/expressions/expression_parser.py", "relevantLinesEnd": 66, "suggestionContent": "The `execute_script` method uses `exec()` which executes arbitrary Python code. Even with `__builtins__` set to None, attackers can escape the sandbox using the same techniques as eval(). Never execute untrusted code with exec().", "oneSentenceSummary": "exec() allows arbitrary code execution even with restricted builtins", "relevantLinesStart": 62}]}}, "metadata" : {}}
{"inputs" : {"inputs": {"filePath": "src/users/user_service.py", "language": "Python", "fileContent": "from dataclasses import dataclass\nfrom datetime import datetime\nfrom typing import Optional\nimport hashlib\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass User:\n    id: str\n    email: str\n    password_hash: str\n    api_key: str\n    created_at: datetime\n\n\n@dataclass\nclass LoginRequest:\n    email: str\n    password: str\n    remember_me: bool = False\n\n\n@dataclass\nclass CreateUserRequest:\n    email: str\n    password: str\n    api_key: str | None = None\n\n\nclass UserService:\n    def __init__(self):\n        self._users: dict[str, User] = {}\n\n    def create_user(self, request: CreateUserRequest) -> User:\n        logger.info(f\"Creating user with email={request.email}, password={request.password}\")\n\n        if request.email in [u.email for u in self._users.values()]:\n            raise ValueError(\"Email already exists\")\n\n        user_id = hashlib.sha256(request.email.encode()).hexdigest()[:16]\n        password_hash = hashlib.sha256(request.password.encode()).hexdigest()\n        api_key = request.api_key or hashlib.sha256(\n            f\"{request.email}:{datetime.now().isoformat()}\".encode()\n        ).hexdigest()\n\n        user = User(\n            id=user_id,\n            email=request.email,\n            password_hash=password_hash,\n            api_key=api_key,\n            created_at=datetime.now()\n        )\n\n        self._users[user_id] = user\n        logger.info(f\"Created user: {user}\")\n\n        return user\n\n    def authenticate(self, request: LoginRequest) -> User | None:\n        logger.debug(f\"Authentication attempt: {request}\")\n\n        for user in self._users.values():\n            if user.email == request.email:\n                password_hash = hashlib.sha256(request.password.encode()).hexdigest()\n                if user.password_hash == password_hash:\n                    logger.info(f\"User authenticated: {user.email}\")\n                    return user\n                else:\n                    logger.warning(f\"Invalid password for user {request.email}: {request.password}\")\n                    return None\n\n        logger.warning(f\"User not found: {request.email}\")\n        return None\n\n    def get_user_by_api_key(self, api_key: str) -> User | None:\n        logger.debug(f\"Looking up user by API key: {api_key}\")\n\n        for user in self._users.values():\n            if user.api_key == api_key:\n                return user\n        return None\n\n    def update_password(self, user_id: str, old_password: str, new_password: str) -> bool:\n        logger.info(f\"Password update request for user {user_id}: old={old_password}, new={new_password}\")\n\n        user = self._users.get(user_id)\n        if not user:\n            return False\n\n        old_hash = hashlib.sha256(old_password.encode()).hexdigest()\n        if user.password_hash != old_hash:\n            return False\n\n        user.password_hash = hashlib.sha256(new_password.encode()).hexdigest()\n        return True\n\n    def get_all_users(self) -> list[User]:\n        logger.info(f\"Returning all users: {list(self._users.values())}\")\n        return list(self._users.values())\n", "pullRequest": {"id": 9999999999, "url": "https://api.github.com/repos/acme/app/pulls/1", "base": {"ref": "main", "repo": {"fullName": "acme/app"}}, "head": {"ref": "feature-branch", "repo": {"fullName": "acme/app"}}}, "groupingMode": "full", "reviewOptions": {"security": true, "code_style": false, "kody_rules": true, "refactoring": true, "error_handling": true, "maintainability": true, "potential_issues": true, "documentation_and_comments": false}, "limitationType": "pr", "patchWithLinesStr": "## file: 'src/users/user_service.py'\n\n@@ -0,0 +1,103 @@\n__new hunk__\n1 +from dataclasses import dataclass\n2 +from datetime import datetime\n3 +from typing import Optional\n4 +import hashlib\n5 +import logging\n6 +\n7 +logger = logging.getLogger(__name__)\n8 +\n9 +\n10 +@dataclass\n11 +class User:\n12 +    id: str\n13 +    email: str\n14 +    password_hash: str\n15 +    api_key: str\n16 +    created_at: datetime\n17 +\n18 +\n19 +@dataclass\n20 +class LoginRequest:\n21 +    email: str\n22 +    password: str\n23 +    remember_me: bool = False\n24 +\n25 +\n26 +@dataclass\n27 +class CreateUserRequest:\n28 +    email: str\n29 +    password: str\n30 +    api_key: str | None = None\n31 +\n32 +\n33 +class UserService:\n34 +    def __init__(self):\n35 +        self._users: dict[str, User] = {}\n36 +\n37 +    def create_user(self, request: CreateUserRequest) -> User:\n38 +        logger.info(f\"Creating user with email={request.email}, password={request.password}\")\n39 +\n40 +        if request.email in [u.email for u in self._users.values()]:\n41 +            raise ValueError(\"Email already exists\")\n42 +\n43 +        user_id = hashlib.sha256(request.email.encode()).hexdigest()[:16]\n44 +        password_hash = hashlib.sha256(request.password.encode()).hexdigest()\n45 +        api_key = request.api_key or hashlib.sha256(\n46 +            f\"{request.email}:{datetime.now().isoformat()}\".encode()\n47 +        ).hexdigest()\n48 +\n49 +        user = User(\n50 +            id=user_id,\n51 +            email=request.email,\n52 +            password_hash=password_hash,\n53 +            api_key=api_key,\n54 +            created_at=datetime.now()\n55 +        )\n56 +\n57 +        self._users[user_id] = user\n58 +        logger.info(f\"Created user: {user}\")\n59 +\n60 +        return user\n61 +\n62 +    def authenticate(self, request: LoginRequest) -> User | None:\n63 +        logger.debug(f\"Authentication attempt: {request}\")\n64 +\n65 +        for user in self._users.values():\n66 +            if user.email == request.email:\n67 +                password_hash = hashlib.sha256(request.password.encode()).hexdigest()\n68 +                if user.password_hash == password_hash:\n69 +                    logger.info(f\"User authenticated: {user.email}\")\n70 +                    return user\n71 +                else:\n72 +                    logger.warning(f\"Invalid password for user {request.email}: {request.password}\")\n73 +                    return None\n74 +\n75 +        logger.warning(f\"User not found: {request.email}\")\n76 +        return None\n77 +\n78 +    def get_user_by_api_key(self, api_key: str) -> User | None:\n79 +        logger.debug(f\"Looking up user by API key: {api_key}\")\n80 +\n81 +        for user in self._users.values():\n82 +            if user.api_key == api_key:\n83 +                return user\n84 +        return None\n85 +\n86 +    def update_password(self, user_id: str, old_password: str, new_password: str) -> bool:\n87 +        logger.info(f\"Password update request for user {user_id}: old={old_password}, new={new_password}\")\n88 +\n89 +        user = self._users.get(user_id)\n90 +        if not user:\n91 +            return False\n92 +\n93 +        old_hash = hashlib.sha256(old_password.encode()).hexdigest()\n94 +        if user.password_hash != old_hash:\n95 +            return False\n96 +\n97 +        user.password_hash = hashlib.sha256(new_password.encode()).hexdigest()\n98 +        return True\n99 +\n100 +    def get_all_users(self) -> list[User]:\n101 +        logger.info(f\"Returning all users: {list(self._users.values())}\")\n102 +        return list(self._users.values())\n103 +\n__old hunk__\n", "severityLevelFilter": "critical", "languageResultPrompt": "en-US", "maxSuggestionsParams": 5, "organizationAndTeamData": {"teamId": "00000000-0000-0000-0000-000000000000", "organizationId": "00000000-0000-0000-0000-000000000001"}}}, "outputs" : {"reference_outputs": {"overallSummary": "Found 3 bug(s) in src/users/user_service.py", "codeSuggestions": [{"label": "bug", "language": "Python", "existingCode": "logger.info(f\"Creating user with email={request.email}, password={request.password}\")", "improvedCode": "logger.info(f\"Creating user with email={request.email}\")", "relevantFile": "src/users/user_service.py", "relevantLinesEnd": 38, "suggestionContent": "Multiple log statements expose sensitive data including plaintext passwords, API keys, and user objects containing password hashes. Log files are often stored with less security than databases and may be accessible to more personnel. Never log passwords, API keys, tokens, or other secrets.", "oneSentenceSummary": "Logging sensitive data (passwords, API keys) exposes secrets in log files", "relevantLinesStart": 38}, {"label": "bug", "language": "Python", "existingCode": "logger.warning(f\"Invalid password for user {request.email}: {request.password}\")", "improvedCode": "logger.warning(f\"Invalid password for user {request.email}\")", "relevantFile": "src/users/user_service.py", "relevantLinesEnd": 70, "suggestionContent": "The `authenticate` method logs the password when authentication fails. This exposes user passwords in plain text in log files. Combined with the email, an attacker with log access can obtain valid credentials.", "oneSentenceSummary": "Failed login logs expose plaintext password in log files", "relevantLinesStart": 70}, {"label": "bug", "language": "Python", "existingCode": "logger.info(f\"Returning all users: {list(self._users.values())}\")", "improvedCode": "logger.info(f\"Returning {len(self._users)} users\")", "relevantFile": "src/users/user_service.py", "relevantLinesEnd": 97, "suggestionContent": "The `get_all_users` method logs the complete list of User objects, which includes password hashes and API keys. Even hashed passwords shouldn't be logged as they can be subject to offline cracking attempts.", "oneSentenceSummary": "Logging user objects exposes password hashes and API keys", "relevantLinesStart": 97}]}}, "metadata" : {}}
{"inputs" : {"inputs": {"filePath": "src/api/request_handler.py", "language": "Python", "fileContent": "import json\nimport traceback\nfrom typing import Any\nfrom dataclasses import dataclass\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass RequestError(Exception):\n    def __init__(self, message: str, status_code: int = 400):\n        super().__init__(message)\n        self.status_code = status_code\n\n\nclass ValidationError(RequestError):\n    def __init__(self, field: str, message: str):\n        super().__init__(f\"Validation error on {field}: {message}\", 422)\n        self.field = field\n\n\nclass AuthenticationError(RequestError):\n    def __init__(self, message: str = \"Authentication required\"):\n        super().__init__(message, 401)\n\n\n@dataclass\nclass Request:\n    method: str\n    path: str\n    headers: dict[str, str]\n    body: str | None = None\n\n\n@dataclass\nclass Response:\n    status_code: int\n    body: dict[str, Any]\n    headers: dict[str, str] = None\n\n    def __post_init__(self):\n        if self.headers is None:\n            self.headers = {}\n\n\nclass RequestHandler:\n    def __init__(self):\n        self.middleware = []\n\n    def add_middleware(self, middleware_fn):\n        self.middleware.append(middleware_fn)\n\n    def parse_json_body(self, request: Request) -> dict:\n        if not request.body:\n            return {}\n\n        try:\n            return json.loads(request.body)\n        except:\n            raise ValidationError(\"body\", \"Invalid JSON\")\n\n    def validate_required_fields(self, data: dict, required: list[str]):\n        for field in required:\n            if field not in data:\n                raise ValidationError(field, \"Field is required\")\n\n    def handle_request(self, request: Request) -> Response:\n        try:\n            for middleware in self.middleware:\n                request = middleware(request)\n\n            body = self.parse_json_body(request)\n\n            if request.method == \"POST\":\n                self.validate_required_fields(body, [\"name\", \"email\"])\n\n            return Response(\n                status_code=200,\n                body={\"success\": True, \"data\": body}\n            )\n\n        except ValidationError as e:\n            return Response(\n                status_code=e.status_code,\n                body={\"error\": str(e), \"field\": e.field}\n            )\n        except RequestError as e:\n            return Response(\n                status_code=e.status_code,\n                body={\"error\": str(e)}\n            )\n        except:\n            logger.error(f\"Unexpected error: {traceback.format_exc()}\")\n            return Response(\n                status_code=500,\n                body={\"error\": \"Internal server error\"}\n            )\n\n    def process_batch(self, requests: list[Request]) -> list[Response]:\n        responses = []\n        for request in requests:\n            try:\n                response = self.handle_request(request)\n                responses.append(response)\n            except Exception as e:\n                logger.error(f\"Batch request failed: {e}\")\n                responses.append(Response(500, {\"error\": \"Failed\"}))\n        return responses\n\n\ndef safe_divide(a: float, b: float) -> float:\n    try:\n        return a / b\n    except ZeroDivisionError:\n        return 0.0\n    except:\n        return None\n\n\ndef parse_config(config_str: str) -> dict:\n    try:\n        config = json.loads(config_str)\n        return config\n    except json.JSONDecodeError as e:\n        raise ValueError(f\"Invalid config: {e}\")\n    except Exception:\n        raise\n", "pullRequest": {"id": 9999999999, "url": "https://api.github.com/repos/acme/app/pulls/1", "base": {"ref": "main", "repo": {"fullName": "acme/app"}}, "head": {"ref": "feature-branch", "repo": {"fullName": "acme/app"}}}, "groupingMode": "full", "reviewOptions": {"security": true, "code_style": false, "kody_rules": true, "refactoring": true, "error_handling": true, "maintainability": true, "potential_issues": true, "documentation_and_comments": false}, "limitationType": "pr", "patchWithLinesStr": "## file: 'src/api/request_handler.py'\n\n@@ -0,0 +1,128 @@\n__new hunk__\n1 +import json\n2 +import traceback\n3 +from typing import Any\n4 +from dataclasses import dataclass\n5 +import logging\n6 +\n7 +logger = logging.getLogger(__name__)\n8 +\n9 +\n10 +class RequestError(Exception):\n11 +    def __init__(self, message: str, status_code: int = 400):\n12 +        super().__init__(message)\n13 +        self.status_code = status_code\n14 +\n15 +\n16 +class ValidationError(RequestError):\n17 +    def __init__(self, field: str, message: str):\n18 +        super().__init__(f\"Validation error on {field}: {message}\", 422)\n19 +        self.field = field\n20 +\n21 +\n22 +class AuthenticationError(RequestError):\n23 +    def __init__(self, message: str = \"Authentication required\"):\n24 +        super().__init__(message, 401)\n25 +\n26 +\n27 +@dataclass\n28 +class Request:\n29 +    method: str\n30 +    path: str\n31 +    headers: dict[str, str]\n32 +    body: str | None = None\n33 +\n34 +\n35 +@dataclass\n36 +class Response:\n37 +    status_code: int\n38 +    body: dict[str, Any]\n39 +    headers: dict[str, str] = None\n40 +\n41 +    def __post_init__(self):\n42 +        if self.headers is None:\n43 +            self.headers = {}\n44 +\n45 +\n46 +class RequestHandler:\n47 +    def __init__(self):\n48 +        self.middleware = []\n49 +\n50 +    def add_middleware(self, middleware_fn):\n51 +        self.middleware.append(middleware_fn)\n52 +\n53 +    def parse_json_body(self, request: Request) -> dict:\n54 +        if not request.body:\n55 +            return {}\n56 +\n57 +        try:\n58 +            return json.loads(request.body)\n59 +        except:\n60 +            raise ValidationError(\"body\", \"Invalid JSON\")\n61 +\n62 +    def validate_required_fields(self, data: dict, required: list[str]):\n63 +        for field in required:\n64 +            if field not in data:\n65 +                raise ValidationError(field, \"Field is required\")\n66 +\n67 +    def handle_request(self, request: Request) -> Response:\n68 +        try:\n69 +            for middleware in self.middleware:\n70 +                request = middleware(request)\n71 +\n72 +            body = self.parse_json_body(request)\n73 +\n74 +            if request.method == \"POST\":\n75 +                self.validate_required_fields(body, [\"name\", \"email\"])\n76 +\n77 +            return Response(\n78 +                status_code=200,\n79 +                body={\"success\": True, \"data\": body}\n80 +            )\n81 +\n82 +        except ValidationError as e:\n83 +            return Response(\n84 +                status_code=e.status_code,\n85 +                body={\"error\": str(e), \"field\": e.field}\n86 +            )\n87 +        except RequestError as e:\n88 +            return Response(\n89 +                status_code=e.status_code,\n90 +                body={\"error\": str(e)}\n91 +            )\n92 +        except:\n93 +            logger.error(f\"Unexpected error: {traceback.format_exc()}\")\n94 +            return Response(\n95 +                status_code=500,\n96 +                body={\"error\": \"Internal server error\"}\n97 +            )\n98 +\n99 +    def process_batch(self, requests: list[Request]) -> list[Response]:\n100 +        responses = []\n101 +        for request in requests:\n102 +            try:\n103 +                response = self.handle_request(request)\n104 +                responses.append(response)\n105 +            except Exception as e:\n106 +                logger.error(f\"Batch request failed: {e}\")\n107 +                responses.append(Response(500, {\"error\": \"Failed\"}))\n108 +        return responses\n109 +\n110 +\n111 +def safe_divide(a: float, b: float) -> float:\n112 +    try:\n113 +        return a / b\n114 +    except ZeroDivisionError:\n115 +        return 0.0\n116 +    except:\n117 +        return None\n118 +\n119 +\n120 +def parse_config(config_str: str) -> dict:\n121 +    try:\n122 +        config = json.loads(config_str)\n123 +        return config\n124 +    except json.JSONDecodeError as e:\n125 +        raise ValueError(f\"Invalid config: {e}\")\n126 +    except Exception:\n127 +        raise\n128 +\n__old hunk__\n", "severityLevelFilter": "critical", "languageResultPrompt": "en-US", "maxSuggestionsParams": 5, "organizationAndTeamData": {"teamId": "00000000-0000-0000-0000-000000000000", "organizationId": "00000000-0000-0000-0000-000000000001"}}}, "outputs" : {"reference_outputs": {"overallSummary": "Found 2 bug(s) in src/api/request_handler.py", "codeSuggestions": [{"label": "bug", "language": "Python", "existingCode": "except:\n            logger.error(f\"Unexpected error: {traceback.format_exc()}\")\n            return Response(\n                status_code=500,\n                body={\"error\": \"Internal server error\"}\n            )", "improvedCode": "except Exception:\n            logger.error(f\"Unexpected error: {traceback.format_exc()}\")\n            return Response(\n                status_code=500,\n                body={\"error\": \"Internal server error\"}\n            )", "relevantFile": "src/api/request_handler.py", "relevantLinesEnd": 91, "suggestionContent": "Multiple bare `except:` clauses catch all exceptions including `KeyboardInterrupt`, `SystemExit`, and `GeneratorExit`. This prevents proper program termination and can mask critical errors. Use `except Exception:` to catch only regular exceptions, or catch specific exception types.", "oneSentenceSummary": "Bare except clause catches KeyboardInterrupt and SystemExit preventing proper shutdown", "relevantLinesStart": 86}, {"label": "bug", "language": "Python", "existingCode": "except:\n            raise ValidationError(\"body\", \"Invalid JSON\")", "improvedCode": "except json.JSONDecodeError:\n            raise ValidationError(\"body\", \"Invalid JSON\")", "relevantFile": "src/api/request_handler.py", "relevantLinesEnd": 57, "suggestionContent": "In `parse_json_body`, the bare `except:` clause catches all exceptions and raises `ValidationError`, losing the original exception context. Also in `safe_divide`, bare `except:` returns `None` which silently hides errors like `TypeError` from invalid inputs.", "oneSentenceSummary": "Bare except hides original error context and catches unintended exceptions", "relevantLinesStart": 56}]}}, "metadata" : {}}
{"inputs" : {"inputs": {"filePath": "src/metrics/counter_service.py", "language": "Python", "fileContent": "import threading\nfrom typing import Dict\nfrom datetime import datetime\nfrom dataclasses import dataclass, field\nimport time\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass MetricPoint:\n    value: float\n    timestamp: datetime\n\n\n@dataclass\nclass Counter:\n    name: str\n    value: int = 0\n    last_updated: datetime = None\n\n    def increment(self, amount: int = 1):\n        self.value += amount\n        self.last_updated = datetime.now()\n\n    def decrement(self, amount: int = 1):\n        self.value -= amount\n        self.last_updated = datetime.now()\n\n    def reset(self):\n        self.value = 0\n        self.last_updated = datetime.now()\n\n\nclass CounterService:\n    def __init__(self):\n        self._counters: Dict[str, Counter] = {}\n        self._lock = threading.Lock()\n\n    def get_or_create(self, name: str) -> Counter:\n        if name not in self._counters:\n            with self._lock:\n                if name not in self._counters:\n                    self._counters[name] = Counter(name=name)\n        return self._counters[name]\n\n    def increment(self, name: str, amount: int = 1):\n        counter = self.get_or_create(name)\n        counter.increment(amount)\n\n    def decrement(self, name: str, amount: int = 1):\n        counter = self.get_or_create(name)\n        counter.decrement(amount)\n\n    def get_value(self, name: str) -> int:\n        counter = self._counters.get(name)\n        return counter.value if counter else 0\n\n    def get_all_values(self) -> Dict[str, int]:\n        return {name: c.value for name, c in self._counters.items()}\n\n    def reset(self, name: str):\n        counter = self._counters.get(name)\n        if counter:\n            counter.reset()\n\n    def reset_all(self):\n        for counter in self._counters.values():\n            counter.reset()\n\n\nclass RateLimiter:\n    def __init__(self, max_requests: int, window_seconds: int):\n        self.max_requests = max_requests\n        self.window_seconds = window_seconds\n        self._requests: Dict[str, list] = {}\n        self._lock = threading.Lock()\n\n    def is_allowed(self, key: str) -> bool:\n        now = time.time()\n        cutoff = now - self.window_seconds\n\n        if key not in self._requests:\n            self._requests[key] = []\n\n        requests = self._requests[key]\n        requests = [t for t in requests if t > cutoff]\n        self._requests[key] = requests\n\n        if len(requests) < self.max_requests:\n            requests.append(now)\n            return True\n\n        return False\n\n    def get_remaining(self, key: str) -> int:\n        now = time.time()\n        cutoff = now - self.window_seconds\n\n        requests = self._requests.get(key, [])\n        valid_requests = [t for t in requests if t > cutoff]\n\n        return max(0, self.max_requests - len(valid_requests))\n\n\nclass MetricsAggregator:\n    def __init__(self):\n        self.metrics: Dict[str, list[MetricPoint]] = {}\n\n    def record(self, name: str, value: float):\n        if name not in self.metrics:\n            self.metrics[name] = []\n\n        self.metrics[name].append(MetricPoint(\n            value=value,\n            timestamp=datetime.now()\n        ))\n\n    def get_average(self, name: str) -> float:\n        points = self.metrics.get(name, [])\n        if not points:\n            return 0.0\n        return sum(p.value for p in points) / len(points)\n", "pullRequest": {"id": 9999999999, "url": "https://api.github.com/repos/acme/app/pulls/1", "base": {"ref": "main", "repo": {"fullName": "acme/app"}}, "head": {"ref": "feature-branch", "repo": {"fullName": "acme/app"}}}, "groupingMode": "full", "reviewOptions": {"security": true, "code_style": false, "kody_rules": true, "refactoring": true, "error_handling": true, "maintainability": true, "potential_issues": true, "documentation_and_comments": false}, "limitationType": "pr", "patchWithLinesStr": "## file: 'src/metrics/counter_service.py'\n\n@@ -0,0 +1,125 @@\n__new hunk__\n1 +import threading\n2 +from typing import Dict\n3 +from datetime import datetime\n4 +from dataclasses import dataclass, field\n5 +import time\n6 +import logging\n7 +\n8 +logger = logging.getLogger(__name__)\n9 +\n10 +\n11 +@dataclass\n12 +class MetricPoint:\n13 +    value: float\n14 +    timestamp: datetime\n15 +\n16 +\n17 +@dataclass\n18 +class Counter:\n19 +    name: str\n20 +    value: int = 0\n21 +    last_updated: datetime = None\n22 +\n23 +    def increment(self, amount: int = 1):\n24 +        self.value += amount\n25 +        self.last_updated = datetime.now()\n26 +\n27 +    def decrement(self, amount: int = 1):\n28 +        self.value -= amount\n29 +        self.last_updated = datetime.now()\n30 +\n31 +    def reset(self):\n32 +        self.value = 0\n33 +        self.last_updated = datetime.now()\n34 +\n35 +\n36 +class CounterService:\n37 +    def __init__(self):\n38 +        self._counters: Dict[str, Counter] = {}\n39 +        self._lock = threading.Lock()\n40 +\n41 +    def get_or_create(self, name: str) -> Counter:\n42 +        if name not in self._counters:\n43 +            with self._lock:\n44 +                if name not in self._counters:\n45 +                    self._counters[name] = Counter(name=name)\n46 +        return self._counters[name]\n47 +\n48 +    def increment(self, name: str, amount: int = 1):\n49 +        counter = self.get_or_create(name)\n50 +        counter.increment(amount)\n51 +\n52 +    def decrement(self, name: str, amount: int = 1):\n53 +        counter = self.get_or_create(name)\n54 +        counter.decrement(amount)\n55 +\n56 +    def get_value(self, name: str) -> int:\n57 +        counter = self._counters.get(name)\n58 +        return counter.value if counter else 0\n59 +\n60 +    def get_all_values(self) -> Dict[str, int]:\n61 +        return {name: c.value for name, c in self._counters.items()}\n62 +\n63 +    def reset(self, name: str):\n64 +        counter = self._counters.get(name)\n65 +        if counter:\n66 +            counter.reset()\n67 +\n68 +    def reset_all(self):\n69 +        for counter in self._counters.values():\n70 +            counter.reset()\n71 +\n72 +\n73 +class RateLimiter:\n74 +    def __init__(self, max_requests: int, window_seconds: int):\n75 +        self.max_requests = max_requests\n76 +        self.window_seconds = window_seconds\n77 +        self._requests: Dict[str, list] = {}\n78 +        self._lock = threading.Lock()\n79 +\n80 +    def is_allowed(self, key: str) -> bool:\n81 +        now = time.time()\n82 +        cutoff = now - self.window_seconds\n83 +\n84 +        if key not in self._requests:\n85 +            self._requests[key] = []\n86 +\n87 +        requests = self._requests[key]\n88 +        requests = [t for t in requests if t > cutoff]\n89 +        self._requests[key] = requests\n90 +\n91 +        if len(requests) < self.max_requests:\n92 +            requests.append(now)\n93 +            return True\n94 +\n95 +        return False\n96 +\n97 +    def get_remaining(self, key: str) -> int:\n98 +        now = time.time()\n99 +        cutoff = now - self.window_seconds\n100 +\n101 +        requests = self._requests.get(key, [])\n102 +        valid_requests = [t for t in requests if t > cutoff]\n103 +\n104 +        return max(0, self.max_requests - len(valid_requests))\n105 +\n106 +\n107 +class MetricsAggregator:\n108 +    def __init__(self):\n109 +        self.metrics: Dict[str, list[MetricPoint]] = {}\n110 +\n111 +    def record(self, name: str, value: float):\n112 +        if name not in self.metrics:\n113 +            self.metrics[name] = []\n114 +\n115 +        self.metrics[name].append(MetricPoint(\n116 +            value=value,\n117 +            timestamp=datetime.now()\n118 +        ))\n119 +\n120 +    def get_average(self, name: str) -> float:\n121 +        points = self.metrics.get(name, [])\n122 +        if not points:\n123 +            return 0.0\n124 +        return sum(p.value for p in points) / len(points)\n125 +\n__old hunk__\n", "severityLevelFilter": "critical", "languageResultPrompt": "en-US", "maxSuggestionsParams": 5, "organizationAndTeamData": {"teamId": "00000000-0000-0000-0000-000000000000", "organizationId": "00000000-0000-0000-0000-000000000001"}}}, "outputs" : {"reference_outputs": {"overallSummary": "Found 2 bug(s) in src/metrics/counter_service.py", "codeSuggestions": [{"label": "bug", "language": "Python", "existingCode": "def increment(self, name: str, amount: int = 1):\n        counter = self.get_or_create(name)\n        counter.increment(amount)", "improvedCode": "def increment(self, name: str, amount: int = 1):\n        with self._lock:\n            counter = self.get_or_create(name)\n            counter.increment(amount)", "relevantFile": "src/metrics/counter_service.py", "relevantLinesEnd": 49, "suggestionContent": "The `Counter.increment` and `Counter.decrement` methods have a race condition. The operation `self.value += amount` is not atomic - it reads the value, adds, then writes. Multiple threads incrementing simultaneously can lose updates. The `CounterService.increment` calls counter methods outside any lock protection.", "oneSentenceSummary": "Race condition in counter increment - += is not atomic, updates can be lost", "relevantLinesStart": 47}, {"label": "bug", "language": "Python", "existingCode": "def is_allowed(self, key: str) -> bool:\n        now = time.time()\n        cutoff = now - self.window_seconds\n\n        if key not in self._requests:\n            self._requests[key] = []\n\n        requests = self._requests[key]\n        requests = [t for t in requests if t > cutoff]\n        self._requests[key] = requests\n\n        if len(requests) < self.max_requests:\n            requests.append(now)\n            return True\n\n        return False", "improvedCode": "def is_allowed(self, key: str) -> bool:\n        with self._lock:\n            now = time.time()\n            cutoff = now - self.window_seconds\n\n            if key not in self._requests:\n                self._requests[key] = []\n\n            requests = self._requests[key]\n            requests = [t for t in requests if t > cutoff]\n            self._requests[key] = requests\n\n            if len(requests) < self.max_requests:\n                requests.append(now)\n                return True\n\n            return False", "relevantFile": "src/metrics/counter_service.py", "relevantLinesEnd": 92, "suggestionContent": "The `RateLimiter.is_allowed` method has a race condition. The check-then-act sequence (check if under limit, then append) is not atomic. Multiple threads can pass the check simultaneously and all append, exceeding the rate limit. The lock is defined but never used in `is_allowed`.", "oneSentenceSummary": "Race condition in rate limiter - lock defined but not used in is_allowed", "relevantLinesStart": 76}]}}, "metadata" : {}}
